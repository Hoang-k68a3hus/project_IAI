# Task 01: T·∫ßng D·ªØ Li·ªáu (Data Layer)

## M·ª•c Ti√™u

X√¢y d·ª±ng pipeline x·ª≠ l√Ω d·ªØ li·ªáu ·ªïn ƒë·ªãnh, c√≥ kh·∫£ nƒÉng t√°i t·∫°o (reproducible) v√† hi·ªáu nƒÉng cao cho h·ªá th·ªëng CF. Pipeline n√†y s·∫Ω chuy·ªÉn ƒë·ªïi raw CSV th√†nh c√°c ƒë·ªãnh d·∫°ng t·ªëi ∆∞u cho training v√† serving, t·∫≠n d·ª•ng **Explicit Feedback (Rating)** v√† **Rich Metadata** ƒë·ªÉ t·∫°o ƒë·∫ßu v√†o ch·∫•t l∆∞·ª£ng cao cho ALS, BPR v√† PhoBERT.

## üîÑ Key Strategy Changes (Updated November 2025)

### Data Challenges Addressed:
1. **High Sparsity**: ~1.23 interactions/user (369K reviews / 300K users)
   - Most users are one-time buyers ‚Üí CF has minimal overlap to learn from
   - **Trainable Users**: ~26,000 users with ‚â•2 interactions (~8.6% of total)
   - **Matrix Density**: ~0.11% for CF training (26k√ó2.2k with ~65k interactions)
2. **Rating Skew**: ~95% ratings are 5-star ‚Üí Loss of discriminative power
   - Can't distinguish "truly loved" vs "just okay" products

### 1. Sentiment-Based Confidence Weighting
- **Problem**: 5-star ratings lack nuance when everyone gives 5 stars
- **Solution**: Enhance confidence scores using review comment quality
  - Base confidence = rating value (1-5)
  - **+0.2**: Comment length >10 words (thoughtful feedback)
  - **+0.3**: Contains positive keywords ("th·∫•m nhanh", "hi·ªáu qu·∫£", "th∆°m")
  - **+0.5**: Includes images (if data available)
  - **Result**: `confidence_score` = rating + quality_bonus (max ~6.0)
- **Usage**: ALS uses `confidence_score` instead of raw ratings

### 2. User Segmentation Strategy (UPDATED - Lowered to ‚â•2)
- **Trainable Users** (‚â•2 interactions):
  - Use for CF training (ALS/BPR) - minimum data for collaborative patterns
  - **~26,000 users (~8.6% of total)** - sufficient statistical base with BERT support
  - **Critical**: ALS must use BERT initialization + higher regularization (Œª=0.1) to anchor sparse vectors
- **Cold-Start Users** (1 interaction or new users):
  - ~90% of users - skip CF training for these
  - Serve with content-based (PhoBERT item similarity) + popularity
- **Rationale**: Balance data hunger vs quality; BERT embeddings compensate for sparsity

### 3. Hard Negative Mining for BPR
- **Strategy 1**: Low ratings (‚â§3) as explicit hard negatives (if available)
- **Strategy 2**: Implicit hard negatives from popularity
  - Sample from Top-50 popular items user DIDN'T buy
  - Logic: "This product is hot, but you didn't buy it ‚Üí you don't like it"
  - More informative than random negatives

### 4. Content-First Hybrid Approach
- **Shift**: Increase content-based weight (PhoBERT) relative to CF
  - For sparse data, semantic similarity more reliable than collaborative patterns
  - Recommended weights: `w_content=0.4`, `w_cf=0.3`, `w_popularity=0.2`, `w_quality=0.1`

### Legacy Strategy (Preserved for Reference):
- **ALS**: S·ª≠ d·ª•ng rating values (1-5) tr·ª±c ti·∫øp l√†m confidence scores thay v√¨ binary matrix
- **BPR**: Hard Negative Mining - T·∫≠n d·ª•ng low ratings (‚â§3) l√†m negative examples thay v√¨ ch·ªâ random sampling
- **Test Set**: Ch·ªâ ch·ª©a positive interactions (rating ‚â•4) ƒë·ªÉ ƒëo kh·∫£ nƒÉng recommend items user s·∫Ω th√≠ch



## Input Data Sources

### Raw CSV Files
T·∫•t c·∫£ n·∫±m trong `data/published_data/`:

1. **data_reviews_purchase.csv**
   - Columns: `user_id`, `product_id`, `rating`, `comment`, `processed_comment`, `cmt_date`
   - Rows: ~369K interactions
   - **Note**: Uses `processed_comment` column (not `comment`) for sentiment analysis
   - Issues: Tr√πng l·∫∑p user-item, missing timestamps, inconsistent types

2. **data_product.csv**
   - Columns: `product_id`, `product_name`, `brand`, `type`, `price`, `avg_star`, `num_sold_time`, `processed_description`
   - Rows: 2,244 products
   - Usage: Popularity baseline, metadata enrichment

3. **data_product_attribute.csv**
   - Columns: `product_id`, `ingredient`, `feature`, `skin_type`, `capacity`, `design`, `brand`, `expiry`, `origin`
   - Rows: 2,244 products
   - Usage: Attribute-based filtering, reranking signals

4. **data_shop.csv**
   - Columns: Shop metadata
   - Usage: Optional shop-level features (future)

## Preprocessing Steps

### Step 1: Data Validation & Cleaning

#### 1.1 Load v√† Audit
- **Encoding**: ƒê·ªçc CSV v·ªõi `encoding='utf-8'` (Vietnamese characters)
- **Type enforcement**:
  - `user_id`, `product_id`: int
  - `rating`: float (validate range 1.0-5.0)
  - `cmt_date`: parse th√†nh datetime (format: DD/MM/YYYY ho·∫∑c auto-detect)
- **Missing values & "Time Travel" Fix**:
  - Drop rows v·ªõi missing `user_id`, `product_id`, `rating`
  - **CRITICAL**: Drop rows v·ªõi `cmt_date` = NaT/Null (KH√îNG ƒëi·ªÅn placeholder)
    - L√Ω do: Tr√°nh data leakage khi chia train/test - m√¥ h√¨nh kh√¥ng ƒë∆∞·ª£c "nh√¨n th·∫•y t∆∞∆°ng lai"
    - Log: S·ªë l∆∞·ª£ng rows b·ªã drop do missing timestamp
- **Rating validation**:
  - Ch·ªâ gi·ªØ rows v·ªõi `rating` trong kho·∫£ng [1.0, 5.0]
  - Drop ho√†n to√†n c√°c gi√° tr·ªã ngo√†i range (kh√¥ng impute)
  - Log: S·ªë l∆∞·ª£ng invalid ratings removed

#### 1.2 Deduplication
- **Rule**: M·ªói (user_id, product_id) ch·ªâ gi·ªØ 1 interaction
- **Strategy**: Gi·ªØ interaction c√≥ `cmt_date` m·ªõi nh·∫•t
- **Fallback**: N·∫øu `cmt_date` tr√πng ‚Üí gi·ªØ rating cao nh·∫•t
- **Log**: S·ªë l∆∞·ª£ng duplicates removed

#### 1.3 Outlier Detection
- **User activity**: Identify users v·ªõi >500 interactions (potential bots/scrapers)
- **Item popularity**: Flag items v·ªõi <3 interactions (very cold items)
- **Rating distribution**: Check for rating bias (e.g., >90% ratings = 5)
- **Action**: Log outliers, quy·∫øt ƒë·ªãnh filter sau

#### 1.4 Chu·∫©n h√≥a & S·ª≠a l·ªói ch√≠nh t·∫£ (@data)
- **M·ª•c ti√™u**: L√†m s·∫°ch `processed_comment` tr∆∞·ªõc khi ch·∫°y sentiment ƒë·ªÉ gi·∫£m nhi·ªÖu t·ª´ teencode, vi·∫øt t·∫Øt v√† l·ªói g√µ.
- **Ngu·ªìn script**: Th∆∞ m·ª•c `data/` ch·ª©a ƒë·∫ßy ƒë·ªß c√¥ng c·ª•:
  - `apply_spelling_corrections.py`: Chu·∫©n h√≥a teencode, d·∫•u c√¢u, vi·∫øt hoa ƒë·∫ßu c√¢u.
  - `apply_abbreviation_corrections.py` + `apply_abbreviation_corrections_v2.py`: M·ªü r·ªông vi·∫øt t·∫Øt ph·ªï bi·∫øn trong review m·ªπ ph·∫©m.
  - `apply_full_spelling_corrections.py`, `merge_corrections.py`: H·ª£p nh·∫•t b·∫£ng thay th·∫ø t√πy ch·ªânh, b·ªï sung rules ƒë·∫∑c th√π th∆∞∆°ng hi·ªáu.
  - `split_word_frequency.py`, `analyze_underscore_words.py`: H·ªó tr·ª£ th·ªëng k√™ ƒë·ªÉ c·∫≠p nh·∫≠t dictionary.
- **Quy tr√¨nh**:
  1. T·∫°o/ c·∫≠p nh·∫≠t file mapping trong `data/processed/` (v√≠ d·ª• `typo_mapping.json`).
  2. Ch·∫°y pipeline s·ª≠a l·ªói:
     ```bash
     python data/apply_spelling_corrections.py \
       --input data/published_data/data_reviews_purchase.csv \
       --output data/processed/data_reviews_spell_fixed.csv \
       --mapping data/processed/typo_mapping.json
     ```
  3. L·∫∑p l·∫°i v·ªõi script vi·∫øt t·∫Øt (`apply_abbreviation_corrections*.py`) ƒë·ªÉ ƒë·∫£m b·∫£o c√°c c·ª•m nh∆∞ "spf", "msm" ƒë∆∞·ª£c chu·∫©n h√≥a.
  4. D√πng `verify_integrity.py` ƒë·ªÉ so kh·ªõp s·ªë d√≤ng tr∆∞·ªõc/ sau khi s·ª≠a.
- **K·∫øt qu·∫£**: C·ªôt `processed_comment` ƒë∆∞·ª£c thay th·∫ø b·∫±ng phi√™n b·∫£n ƒë√£ chu·∫©n h√≥a, l∆∞u trong `data/processed/` v√† n·∫°p l·∫°i ·ªü Step 2 cho sentiment.

**‚úÖ UPDATED**: `DataValidator` must enforce strict temporal validation and rating range checks

### Step 2: Explicit Feedback Feature Engineering

#### 2.0 Comment Quality Analysis (AI-Powered - Addresses Rating Skew)
- **Problem**: 95% ratings are 5-star ‚Üí need additional signal to distinguish quality
- **Solution**: AI-powered sentiment analysis using ViSoBERT + heuristic adjustments
- **Model**: `5CD-AI/Vietnamese-Sentiment-visobert` (trained on 120K Vietnamese e-commerce reviews)
- **Column**: Uses `processed_comment` column (not `comment`)
  
**Implementation Details** (Updated to Match Code):

- **AI Sentiment Analysis** (`FeatureEngineer._compute_sentiment_batch`): 
  - Uses pre-trained ViSoBERT model for Vietnamese text sentiment
  - **GPU Batch Processing**: Automatic GPU detection, batch_size=64
  - Output: Sentiment probability distribution (NEGATIVE, POSITIVE, NEUTRAL)
  - Converts to quality score [0.0, 1.0] based on positive sentiment probability
  - **Fallback**: Returns 0.5 if model unavailable or text empty
  
- **Fake Review Detection Heuristics** (`FeatureEngineer._apply_fake_review_checks`):
  - **Length Analysis**:
    - Bonus +0.1: Reviews >25 words (thoughtful feedback)
    - Penalty -0.1: Reviews <4 words (too short to be meaningful)
  - **Keyword Matching** (Extended Vietnamese dictionaries):
    - Positive keywords: "th·∫•m nhanh", "hi·ªáu qu·∫£", "th∆°m", "m·ªãn", "s√°ng da", "t·ªët", "∆∞ng", etc.
    - Negative keywords: "k√©m", "d·ªü", "th·∫•t v·ªçng", "fake", "gi·∫£", "t·ªá", "kh√¥ng hi·ªáu qu·∫£", etc.
    - Bonus/penalty ¬±0.15 based on keyword presence
  - **Recency Decay**: Older reviews get slight down-weighting (configurable factor)
  - **Rating-Sentiment Mismatch Detection**:
    - High rating (‚â•4) + low sentiment (<0.3) ‚Üí penalty -0.2 (suspicious)
    - Low rating (‚â§2) + high sentiment (>0.7) ‚Üí penalty -0.2 (suspicious)
  - **Repetition Penalty**: Low character diversity (repetitive text) ‚Üí penalty -0.15
  - **Emoji Sentiment Mapping** (NEW):
    - **Positive emojis**: üòç‚ù§Ô∏èüëç‚ú®üåüüíØüî•üíïüòäü•∞ ‚Üí bonus +0.1
    - **Negative emojis**: üò¢üò≠üíîüëéüò°üò§üòûüòî ‚Üí penalty -0.1
    - **Neutral emojis**: ü§îüòêüò∂ ‚Üí no change

- **Final Quality Score Calculation**:
  ```python
  base_quality = ai_sentiment_score  # [0.0, 1.0]
  adjusted_quality = base_quality + sum(heuristic_adjustments)
  comment_quality = np.clip(adjusted_quality, 0.0, 1.0)
  confidence_score = rating + comment_quality  # [1.0, 6.0]
  ```

**Usage**:
```python
from recsys.cf.data import DataProcessor

processor = DataProcessor(
    positive_threshold=4.0,
    hard_negative_threshold=3.0,
    no_comment_quality=0.5,  # Default for missing comments
    sentiment_model="5CD-AI/Vietnamese-Sentiment-visobert",
    batch_size=64,
    enable_fake_review_checks=True
)

# Compute confidence scores (includes AI sentiment + fake review checks)
df_enriched, stats = processor.compute_comment_quality(
    df_clean,
    comment_column='processed_comment'  # Note: uses processed_comment
)

# Result columns:
# - comment_quality: [0.0, 1.0] quality score (AI + heuristics)
# - confidence_score: rating + comment_quality [1.0, 6.0]

# Stats returned:
# - mean_quality, std_quality, p01, p50, p99
# - num_processed, num_empty, num_with_emoji
# - sentiment_model_used, fake_review_checks_enabled
```

**Quality Score Interpretation**:
- Missing/empty comments: `no_comment_quality` (default 0.5)
- Low quality reviews (spam, fake): 0.0 - 0.3
- Medium quality reviews: 0.3 - 0.7
- High quality reviews (detailed, genuine): 0.7 - 1.0

#### 2.1 ALS: Confidence-Weighted Matrix
- **Paradigm Shift**: S·ª≠ d·ª•ng Explicit Feedback v·ªõi sentiment-based weighting
- **Matrix values**: `confidence_score` = rating + comment_quality (range 1.0-6.0)
  - Rating 5 + quality 0.0 ‚Üí Confidence 5.0 (bare 5-star, suspicious)
  - Rating 5 + quality 1.0 ‚Üí Confidence 6.0 (genuine 5-star with thoughtful review)
  - Rating 3 + quality 0.5 ‚Üí Confidence 3.5 (mediocre but detailed feedback)
- **Rationale**: Distinguish "truly loved" products from "just okay" despite rating skew
- **Alternative**: Normalize to [0,1] ‚Üí `normalized_conf = (confidence - 1) / 5`

#### 2.2 BPR: Positive Labels v·ªõi Hard Negative Mining
- **Positive Signal Definition**: 
  - `rating >= 4` ‚Üí Positive interaction (User th√≠ch s·∫£n ph·∫©m)
  - Store in `is_positive` column (0/1)
  
- **Hard Negative Mining (UPDATED for Sparsity)**: 
  - **Strategy 1**: `rating <= 3` ‚Üí Explicit hard negative (User ƒë√£ mua nh∆∞ng th·∫•t v·ªçng)
  - **Strategy 2**: Implicit hard negatives from popularity (NEW)
    - Identify Top-50 most popular items (by `num_sold_time`)
    - For each user, find popular items they DIDN'T interact with
    - Logic: "Hot product but you didn't buy ‚Üí implicit negative preference"
  - Store both in `is_hard_negative` column with source flag
  
- **Sampling Strategy (for BPR training)**:
  - Positive samples: Items v·ªõi `is_positive=1`
  - Negative samples: 30% hard negatives (explicit + implicit) + 70% random unseen
  - Rationale: Combat sparsity with popularity-informed negatives

#### 2.3 User Filtering (UPDATED - Lowered to ‚â•2)
- **Segment users by interaction count**:
  - **Trainable users**: ‚â•2 interactions **AND** ‚â•1 positive (rating ‚â•4)
    - **~26,000 users (~8.6% coverage)** with matrix density ~0.11%
    - These users provide minimal collaborative signal but BERT init compensates
    - Mark with `is_trainable_user = True`
  - **Cold-start users**: 1 interaction or no positives
    - Majority of users (~90%), but insufficient for CF
    - Mark with `is_trainable_user = False`
    - Will be served via content-based + popularity

- **Filtering for CF training**:
  - Train ALS/BPR only on `is_trainable_user = True`
  - **Special case**: User with exactly 2 interactions where both are negative (rating <4) ‚Üí Force to cold-start
  - Log stats:
    - Trainable users: ~26,000 (~8.6% of total)
    - Cold-start users: ~274,000 (~91.4% of total)
    - Interactions from trainable users: ~65,000 (matrix density ~0.11%)

- **Iterative filtering**: Still apply min item interactions (‚â•5 positives) after user filtering

### Step 3: ID Mapping (Contiguous Indexing)

#### 3.1 User Mapping
- **Original**: `user_id` (sparse integers, gaps, range 1-304708)
- **Mapped**: `u_idx` (contiguous 0 to num_users-1)
- **Dict structure**: 
  ```
  {
    "user_to_idx": {original_id: idx, ...},
    "idx_to_user": {idx: original_id, ...}
  }
  ```

#### 3.2 Item Mapping
- **Original**: `product_id` (range 0-2243)
- **Mapped**: `i_idx` (contiguous 0 to num_items-1)
- **Dict structure**: T∆∞∆°ng t·ª± user mapping

#### 3.3 Apply Mapping
- Add columns `u_idx`, `i_idx` v√†o interactions DataFrame
- Validate: Kh√¥ng c√≥ missing mappings

#### 3.4 Save Mappings
- **Format**: JSON (human-readable, d·ªÖ debug)
- **Location**: `data/processed/user_item_mappings.json`
- **Include metadata**:
  - Timestamp t·∫°o mappings
  - S·ªë l∆∞·ª£ng users/items
  - Hash c·ªßa raw data

### Step 4: Temporal Split (Leave-One-Out) - OPTIMIZED

#### 4.1 Sort Per User (Vectorized)
- Group interactions theo `u_idx`
- Sort m·ªói group theo `cmt_date` (ascending)
- Handle ties: N·∫øu `cmt_date` tr√πng ‚Üí sort theo `rating` desc
- **Optimization**: Uses pandas `groupby().rank()` instead of Python loops

#### 4.2 Train/Test Split v·ªõi Positive-Only Test (Vectorized)
- **Train**: T·∫•t c·∫£ interactions tr·ª´ latest positive
- **Test**: Latest **POSITIVE** interaction per user
  - **CRITICAL RULE**: Ch·ªâ ch·ªçn t∆∞∆°ng t√°c cu·ªëi c√πng l√†m test N·∫æU `rating >= 4`
  - N·∫øu latest interaction c√≥ rating < 4 ‚Üí L·∫•y latest positive interaction (rating ‚â•4) tr∆∞·ªõc ƒë√≥
  - Rationale: Test set ƒëo l∆∞·ªùng kh·∫£ nƒÉng recommend items user s·∫Ω **th√≠ch**, kh√¥ng ph·∫£i items user s·∫Ω gh√©t
- **Validation**: Optional - l·∫•y 2nd latest positive l√†m val, remaining positives l√†m train
- **Negative Holdouts** (Optional):
  - Reserve explicit negative interactions (rating ‚â§3) for evaluation
  - Helps measure model's ability to avoid recommending disliked items
- **Implicit Negatives** (For Evaluation):
  - Sample 50 popular items per user that user DIDN'T interact with
  - Strategy: 'popular' (Top-K popular items) or 'random'
  - Used for unbiased offline ranking evaluation (NDCG@K, Recall@K)

**Vectorized Implementation** (10-100x faster than iterative):
```python
# OLD slow approach (per-user loop):
# for u_idx in df['u_idx'].unique():
#     user_df = df[df['u_idx'] == u_idx]
#     latest_positive = user_df[user_df['is_positive']].iloc[-1]
#     ...

# NEW fast vectorized approach:
# 1. Rank interactions by timestamp (descending) within each user
df['rank_desc'] = df.groupby('u_idx')['cmt_date'].rank(
    method='first', ascending=False
)

# 2. Find latest positive per user using vectorized operations
df['positive_rank'] = df.groupby('u_idx').apply(
    lambda g: g[g['is_positive']]['cmt_date'].rank(ascending=False)
).values

# 3. Assign split using boolean indexing (no loops)
df['split'] = 'train'
df.loc[(df['positive_rank'] == 1) & (df['is_positive']), 'split'] = 'test'
```

#### 4.3 Edge Cases (UPDATED for ‚â•2 Threshold)
- **Users with exactly 2 interactions**:
  - Both positive (‚â•4): Keep 1 for train, 1 for test ‚Üí Valid trainable user
  - 1 positive, 1 negative: Keep positive for train, negative excluded ‚Üí Train-only user (no test)
  - Both negative (<4): Force to cold-start (`is_trainable_user = False`)
- **Users with 1 positive**: No test data ‚Üí Skip user in evaluation OR use as train-only
- **Users with 0 positives**: Already removed in Step 2.3
- **Users with latest interaction negative**: Take previous positive for test (if exists)

#### 4.4 Implicit Negative Sampling (For Evaluation)
```python
# Sample 50 popular items per user that user didn't interact with
splitter = TemporalSplitter(
    implicit_negative_per_user=50,
    implicit_negative_strategy='popular'  # or 'random'
)

df_split, split_stats = splitter.split(df_mapped)

# Implicit negatives stored in split_stats:
# split_stats['implicit_negatives'] = {u_idx: Set[i_idx], ...}
```

#### 4.5 Create Datasets
- `train_interactions`: DataFrame v·ªõi columns [u_idx, i_idx, rating, is_positive, is_hard_negative, timestamp]
- `test_interactions`: Ch·ªâ ch·ª©a positive interactions (rating ‚â•4)
- `val_interactions`: Optional, c≈©ng ch·ªâ ch·ª©a positives
- `implicit_negatives`: Dict[u_idx, Set[i_idx]] for evaluation

### Step 5: Matrix Construction

#### 5.1 Dual CSR Matrices
- **X_train_confidence** (for ALS): 
  - Shape: (num_trainable_users, num_items)
  - Values: `confidence_score` (rating + comment_quality, range 1.0-6.0)
  - **Only includes trainable users** (‚â•3 interactions)
  - Library: `scipy.sparse.csr_matrix`
  - Usage: ALS training v·ªõi sentiment-enhanced confidence weighting
  
- **X_train_binary** (for BPR - optional):
  - Shape: (num_trainable_users, num_items)
  - Values: Binary (1 for positive interactions only, 0 elsewhere)
  - Usage: BPR pairwise ranking

#### 5.2 User Positive Sets
- **Structure**: Dict `user_pos_train[u_idx] = set(i_idx, ...)`
- **Scope**: Only trainable users
- **Usage**: 
  - Negative sampling trong BPR (exclude positives)
  - Filtering seen items khi generate recommendations
  - Fast lookup O(1)

#### 5.3 Hard Negative Sets (UPDATED)
- **Structure**: Dict `user_hard_neg_train[u_idx] = {"explicit": set(...), "implicit": set(...)}`
- **Content**: 
  - `explicit`: Items v·ªõi rating ‚â§3 (user ƒë√£ mua nh∆∞ng th·∫•t v·ªçng)
  - `implicit`: Popular items (Top-50) user DIDN'T buy (for training)
- **Usage**: 
  - BPR training: Sample 30% from combined hard negatives, 70% random unseen
  - Evaluation: Implicit negatives (50 per user) used for unbiased ranking metrics
  - Analysis: Understand failure modes
- **Implementation**: 
  - Explicit negatives: From interactions with `rating <= hard_negative_threshold`
  - Implicit negatives: Top-K popular items (by `num_sold_time`) user didn't interact with

#### 5.4 Item Popularity with Top-K Tracking
- **Count**: S·ªë l·∫ßn m·ªói item xu·∫•t hi·ªán trong train
- **Log-transform**: Apply `log(1 + count)`
- **Top-K popular items**: Store indices of Top-50 most popular items
  - Usage: Generate implicit hard negatives for cold-start users
  - Format: `top_k_popular_items = [i_idx1, i_idx2, ..., i_idx50]`

#### 5.5 User Segmentation Metadata
- **Structure**: Dict v·ªõi user statistics
  ```python
  user_metadata = {
      "trainable_users": set(u_idx for users with ‚â•3 interactions),
      "cold_start_users": set(u_idx for users with 1-2 interactions),
      "user_interaction_counts": {u_idx: count, ...}
  }
  ```
- **Usage**: 
  - Serving layer decides CF vs content-based routing
  - Monitoring CF coverage

### Step 6: Save Processed Data

#### 6.1 Parquet Format
- **File**: `data/processed/interactions.parquet`
- **Content**: Full DataFrame v·ªõi columns:
  - `user_id`, `product_id`, `u_idx`, `i_idx`
  - `rating`, `comment_quality`, `confidence_score`
  - `is_positive`, `is_hard_negative`, `timestamp`
  - `is_trainable_user` (NEW - flag for CF training eligibility)
  - `split` (train/val/test)
- **Advantages**: 
  - 10x faster read/write vs CSV
  - Type preservation
  - Compression (~50% size reduction)

#### 6.2 Mappings JSON
- **File**: `data/processed/user_item_mappings.json`
- **Structure**:
  ```json
  {
    "metadata": {
      "created_at": "2025-01-15T10:30:00",
      "num_users": 12000,
      "num_items": 2200,
      "data_hash": "abc123...",
      "positive_threshold": 4,
      "hard_negative_threshold": 3
    },
    "user_to_idx": {...},
    "idx_to_user": {...},
    "item_to_idx": {...},
    "idx_to_item": {...}
  }
  ```

#### 6.3 Matrix Files
- **X_train_confidence.npz**: CSR matrix v·ªõi confidence scores (for ALS) - trainable users only
- **X_train_binary.npz**: CSR matrix v·ªõi binary values (for BPR - optional) - trainable users only
- **user_pos_train.pkl**: Pickle dict v·ªõi positive item sets (trainable users)
- **user_hard_neg_train.pkl**: Pickle dict v·ªõi hard negative item sets (explicit + implicit)
- **item_popularity.npy**: NumPy array v·ªõi log-transformed popularity scores
- **top_k_popular_items.json**: List of Top-50 popular item indices
- **user_metadata.pkl**: User segmentation data (trainable vs cold-start)

#### 6.4 Statistics Summary (UPDATED - Add Global Normalization Ranges)
- **File**: `data/processed/data_stats.json`
- **Content**:
  - Train/val/test sizes
  - Sparsity: nonzeros / (users * items)
  - Rating distribution (mean, std, quantiles per split)
  - Positive vs Hard negative counts
  - User/item interaction histograms (quantiles)
  - Filtered counts (users, items, interactions)
  - **NEW - Global Normalization Ranges** (Critical for Task 08):
    - `popularity`: {"min": X, "max": Y, "p01": A, "p99": B}
    - `comment_quality`: {"min": 0.0, "max": 1.0, "mean": C, "std": D, "p01": E, "p99": F}
    - `confidence_score`: {"min": 1.0, "max": 6.0, "p01": E, "p99": F}
    - `rating`: {"min": 1.0, "max": 5.0, "mean": G, "std": H}
  - **Purpose**: Enable global normalization in hybrid reranking to prevent per-request bias

**Example structure**:
```json
{
  "train_size": 350000,
  "test_size": 15000,
  "sparsity": 0.0012,
  "trainable_users": {
    "count": 26000,
    "percentage": 8.6,
    "avg_interactions_per_user": 2.5,
    "matrix_density": 0.0011
  },
  "popularity": {
    "min": 0.0,
    "max": 9.21,
    "mean": 2.45,
    "std": 1.83,
    "p01": 0.0,
    "p50": 2.1,
    "p99": 7.8
  },
  "comment_quality": {
    "min": 0.0,
    "max": 1.0,
    "mean": 0.65,
    "std": 0.18,
    "p01": 0.3,
    "p50": 0.68,
    "p99": 0.95
  },
  "rating": {
    "min": 1.0,
    "max": 5.0,
    "mean": 4.67,
    "std": 0.52
  },
  "confidence_score": {
    "min": 1.0,
    "max": 6.0,
    "mean": 5.12,
    "std": 0.68,
    "p01": 3.2,
    "p99": 6.0
  }
}
```

### Step 7: Data Versioning

#### 7.1 Hash Calculation
- **Method**: MD5 hash c·ªßa raw CSV files (sorted concatenation)
- **Purpose**: Track data changes, invalidate stale models
- **Storage**: In mappings JSON and model artifacts

#### 7.2 Timestamp Tracking
- **Creation time**: Khi n√†o data ƒë∆∞·ª£c processed
- **Usage**: Detect stale data, schedule retraining

#### 7.3 Version Registry
- **File**: `data/processed/versions.json`
- **Structure**:
  ```json
  {
    "v1": {
      "hash": "abc123",
      "timestamp": "2025-01-15T10:30:00",
      "filters": {"min_user_pos": 2, "min_item_pos": 5},
      "files": ["interactions.parquet", "mappings.json", ...]
    }
  }
  ```

## Output Artifacts

### Primary Files (11 files in `data/processed/`)

1. **`interactions.parquet`** - Full interaction data v·ªõi columns:
   - `user_id`, `product_id`, `u_idx`, `i_idx`
   - `rating`, `comment_quality`, `confidence_score`
   - `is_positive`, `is_hard_negative`, `is_trainable_user`
   - `cmt_date`, `split` (train/val/test)
   - Size: ~50% smaller than CSV, 10x faster load

2. **`user_item_mappings.json`** - ID mappings v·ªõi metadata:
   - `user_to_idx`, `idx_to_user`, `item_to_idx`, `idx_to_item`
   - Metadata: `positive_threshold`, `hard_negative_threshold`, counts, timestamps, data_hash

3. **`X_train_confidence.npz`** - Sparse CSR matrix cho ALS (trainable users only)
   - Shape: (num_trainable_users, num_items)
   - Values: `confidence_score` = rating + comment_quality [1.0, 6.0]
   - Format: `scipy.sparse.csr_matrix` saved via `scipy.sparse.save_npz`

4. **`X_train_binary.npz`** - Sparse CSR matrix cho BPR (optional)
   - Shape: (num_trainable_users, num_items)
   - Values: 1 for positive interactions, 0 elsewhere

5. **`user_pos_train.pkl`** - User positive sets (trainable users)
   - Type: `Dict[int, Set[int]]` (u_idx ‚Üí set of positive i_idx)
   - Usage: Negative sampling, seen-item filtering

6. **`user_hard_neg_train.pkl`** - User hard negative sets (explicit + implicit)
   - Type: `Dict[int, Dict[str, Set[int]]]`
   - Structure: `{u_idx: {"explicit": Set[i_idx], "implicit": Set[i_idx]}}`
   - Explicit: Items with rating ‚â§3
   - Implicit: Top-50 popular items user didn't interact with

7. **`item_popularity.npy`** - Log-transformed popularity distribution
   - Shape: (num_items,)
   - Values: `log(1 + interaction_count)` per item

8. **`top_k_popular_items.json`** - Top-50 popular items for implicit negatives
   - Type: `List[int]` of i_idx
   - Usage: Generate implicit hard negatives for cold-start users

9. **`user_metadata.pkl`** - User segmentation metadata
   - Type: `Dict` with:
     - `trainable_users`: `Set[int]` of trainable u_idx
     - `cold_start_users`: `Set[int]` of cold-start u_idx
     - `user_interaction_counts`: `Dict[int, int]`
     - `statistics`: counts, percentages

### Metadata Files

10. **`data_stats.json`** - Comprehensive statistics summary:
    - Train/val/test sizes, sparsity, matrix density
    - Trainable user statistics (count, percentage, avg interactions)
    - Rating distribution (mean, std, quantiles)
    - Comment quality distribution (mean, std, p01, p99)
    - Confidence score distribution (mean, std, p01, p99)
    - **Global normalization ranges** for hybrid reranking (Task 08)

11. **`versions.json`** - Version tracking registry:
    - Version history with hash, timestamp, filters, files, stats
    - Supports version comparison and staleness detection
    - Git commit tracking for reproducibility

### Logging Files

12. **`logs/cf/data_processing.log`** - Processing logs (UTF-8 encoded)
    - Step-by-step processing logs
    - Quality reports, validation results, statistics
    - Rotation: Keep last 10 runs

### Content Enrichment Files
13. `data/processed/product_attributes_enriched.parquet` - Standardized attributes + auxiliary signals
14. `data/processed/content_based_embeddings/product_embeddings.pt` - PhoBERT embeddings v·ªõi rich text
15. `data/processed/content_based_embeddings/embedding_metadata.json` - Embedding version info

## Quality Checks

### Validation Tests
- [ ] No missing values trong key columns (u_idx, i_idx, rating)
- [ ] No NaT/Null timestamps (strict temporal validation)
- [ ] All ratings trong range [1.0, 5.0]
- [ ] u_idx range = [0, num_users-1], i_idx range = [0, num_items-1]
- [ ] CSR matrices shape matches (num_users, num_items)
- [ ] user_pos_train keys = all u_idx in train v·ªõi positives
- [ ] user_hard_neg_train keys = subset of u_idx v·ªõi hard negatives
- [ ] Test set: 1 positive interaction per user (ho·∫∑c 0 n·∫øu user filtered)
- [ ] No data leakage: Test timestamps > Train timestamps per user
- [ ] Test set only contains positives (rating ‚â•4)
- [ ] Mappings reversible: user_to_idx ‚Üí idx_to_user round-trip OK
- [ ] PhoBERT embeddings align v·ªõi product_id trong mappings
- [ ] skin_type_standardized contains valid list values
- [ ] popularity_score v√† quality_score kh√¥ng c√≥ NaN
- [ ] **NEW**: `processed_comment` column exists (not `comment`)
- [ ] **NEW**: `comment_quality` range [0.0, 1.0] (validated)
- [ ] **NEW**: `confidence_score` = rating + comment_quality (validated)
- [ ] **NEW**: AI sentiment model loaded successfully (if enabled)
- [ ] **NEW**: Temporal split includes implicit negatives (50 per user)

### Performance Benchmarks
- [ ] Parquet load time: <5 seconds cho 369K rows
- [ ] CSR matrix construction: <2 seconds
- [ ] Mapping lookup: O(1) constant time
- [ ] **AI sentiment analysis**: <10 minutes cho 369K comments (GPU batch processing, batch_size=64)
- [ ] **Temporal split (vectorized)**: <10 seconds cho 369K interactions (10-100x faster than iterative)
- [ ] PhoBERT encoding: <10 minutes cho 2244 products
- [ ] **Full pipeline (all 7 steps)**: <15 minutes end-to-end

## Configuration Example

**Python Configuration** (Recommended - matches actual implementation):

```python
from recsys.cf.data import DataProcessor

# Initialize processor with full configuration
processor = DataProcessor(
    # Data paths
    base_path="data/published_data",
    output_path="data/processed",
    
    # Validation settings
    rating_min=1.0,
    rating_max=5.0,
    drop_missing_timestamps=True,  # CRITICAL: No placeholder dates
    
    # Explicit feedback thresholds
    positive_threshold=4.0,  # rating >= 4 ‚Üí positive
    hard_negative_threshold=3.0,  # rating <= 3 ‚Üí hard negative
    
    # Comment quality settings (FeatureEngineer)
    no_comment_quality=0.5,  # Default for missing comments
    sentiment_model="5CD-AI/Vietnamese-Sentiment-visobert",
    batch_size=64,  # GPU batch size for sentiment inference
    enable_fake_review_checks=True,  # Enable heuristic adjustments
    
    # User filtering settings (UserFilter)
    min_user_interactions=2,  # Minimum total interactions for trainable user
    min_user_positives=1,     # Must have at least 1 positive (rating ‚â•4)
    min_item_positives=5,     # Items must have ‚â•5 positive interactions
    
    # Temporal split settings (TemporalSplitter)
    include_negative_holdout=True,  # Reserve explicit negatives for analysis
    implicit_negative_per_user=50,  # Implicit negatives for evaluation
    implicit_negative_strategy='popular',  # 'popular' or 'random'
    
    # Version registry settings
    versions_file="versions.json",
    max_versions_kept=10  # Keep last N versions
)

# Alternative: Individual class usage for fine-grained control
from recsys.cf.data import (
    DataReader, DataAuditor, FeatureEngineer, 
    UserFilter, IDMapper, TemporalSplitter,
    MatrixBuilder, DataSaver, VersionRegistry
)

# Example: Custom FeatureEngineer with specific settings
engineer = FeatureEngineer(
    model_name="5CD-AI/Vietnamese-Sentiment-visobert",
    batch_size=64,
    no_comment_quality=0.5,
    enable_fake_review_checks=True,
    # Custom emoji mappings (optional)
    positive_emojis={'üòç', '‚ù§Ô∏è', 'üëç', '‚ú®', 'üåü', 'üíØ', 'üî•'},
    negative_emojis={'üò¢', 'üò≠', 'üíî', 'üëé', 'üò°', 'üò§'},
    # Custom keyword dictionaries (optional)
    positive_keywords={'th·∫•m nhanh', 'hi·ªáu qu·∫£', 'th∆°m', 'm·ªãn', 's√°ng da'},
    negative_keywords={'k√©m', 'd·ªü', 'th·∫•t v·ªçng', 'fake', 'gi·∫£'}
)

# Example: Custom TemporalSplitter for evaluation
splitter = TemporalSplitter(
    positive_threshold=4.0,
    include_negative_holdout=True,
    implicit_negative_per_user=50,
    implicit_negative_strategy='popular'
)
```

**YAML Configuration** (Alternative - for external config files):

```yaml
# data_config.yaml
raw_data:
  base_path: "data/published_data"
  interactions: "data_reviews_purchase.csv"
  products: "data_product.csv"
  attributes: "attribute_based_embeddings/attribute_text_filtering.csv"

preprocessing:
  # Validation
  rating_min: 1.0
  rating_max: 5.0
  drop_missing_timestamps: true  # CRITICAL: No placeholder dates
  
  # Explicit feedback thresholds
  positive_threshold: 4.0  # rating >= 4 ‚Üí positive
  hard_negative_threshold: 3.0  # rating <= 3 ‚Üí hard negative
  
  # Comment quality (AI sentiment)
  comment_column: "processed_comment"  # Note: uses processed_comment
  no_comment_quality: 0.5  # Default for missing comments
  use_ai_sentiment: true  # Use ViSoBERT model
  model_name: "5CD-AI/Vietnamese-Sentiment-visobert"
  batch_size: 64  # GPU batch size
  
  # Filtering (UPDATED - Lowered to 2 for trainable users)
  min_user_interactions: 2  # Minimum total interactions for trainable user
  min_user_positives: 1  # Must have at least 1 positive (rating ‚â•4)
  min_item_positives: 5  # Items must have ‚â•5 positive interactions
  dedup_strategy: "keep_latest"  # or "keep_highest_rating"

temporal_split:
  method: "leave_one_out"
  test_positive_only: true  # Only use positive interactions for test
  validation: false  # Enable val set?
  include_negative_holdout: true  # Reserve explicit negatives
  implicit_negative_per_user: 0   # Enable only when ranking eval needs it
  implicit_negative_strategy: "popular"  # or "random"

matrix_construction:
  als_matrix: "confidence"  # Use confidence_score (rating + comment_quality)
  bpr_matrix: "binary"  # Binary matrix for BPR (optional)
  hard_negative_sampling_ratio: 0.3  # 30% hard neg, 70% random neg
  top_k_popular: 50  # Top-K popular items for implicit negatives

content_enrichment:
  enable_bert: true
  bert_model: "vinai/phobert-base"
  bert_input_fields: ["product_name", "ingredient", "feature", "skin_type", "brand", "processed_description"]
  standardize_skin_type: true
  compute_auxiliary_signals: true
  log_transform_popularity: true

output:
  processed_path: "data/processed"
  format: "parquet"  # or "csv"
  save_confidence_matrix: true  # X_train_confidence.npz
  save_binary_matrix: false  # X_train_binary.npz (optional)
  save_hard_negatives: true  # user_hard_neg_train.pkl
  save_stats: true
  save_embeddings: true
  
versioning:
  enable: true
  hash_method: "md5"
  registry_path: "data/processed/versions.json"
```

## Module Interface

### Architecture Overview

Code ƒë√£ ƒë∆∞·ª£c refactor th√†nh **class-based architecture** v·ªõi c√°c modules ri√™ng bi·ªát:

```
recsys/cf/data/
‚îú‚îÄ‚îÄ __init__.py                    # Package exports (all classes + convenience functions)
‚îú‚îÄ‚îÄ data.py                        # DataProcessor (main orchestrator, ~1000+ lines)
‚îú‚îÄ‚îÄ README.md                      # Module documentation v·ªõi usage examples
‚îî‚îÄ‚îÄ processing/
    ‚îú‚îÄ‚îÄ __init__.py                # Processing submodule exports
    ‚îú‚îÄ‚îÄ read_data.py               # DataReader class (CSV loading, UTF-8)
    ‚îú‚îÄ‚îÄ audit_data.py              # DataAuditor class (validation, dedup, outliers)
    ‚îú‚îÄ‚îÄ feature_engineering.py     # FeatureEngineer class (ViSoBERT sentiment, fake review detection)
    ‚îú‚îÄ‚îÄ user_filtering.py          # UserFilter class (trainable/cold-start segmentation)
    ‚îú‚îÄ‚îÄ id_mapping.py              # IDMapper class (bidirectional mappings)
    ‚îú‚îÄ‚îÄ temporal_split.py          # TemporalSplitter class (optimized vectorized split)
    ‚îú‚îÄ‚îÄ matrix_construction.py     # MatrixBuilder class (CSR matrices, user sets)
    ‚îú‚îÄ‚îÄ data_saver.py              # DataSaver class (Parquet, JSON, NPZ, PKL)
    ‚îî‚îÄ‚îÄ version_registry.py        # VersionRegistry class (versioning, comparison)
```

**Note**: `als_data.py` v√† `bpr_data.py` ƒë√£ ƒë∆∞·ª£c merged v√†o `data.py` v√† `matrix_construction.py`.

### Main Class: `DataProcessor` (`recsys/cf/data/data.py`)

**Unified interface** k·∫øt h·ª£p t·∫•t c·∫£ processing steps (~1000+ lines):

```python
from recsys.cf.data import DataProcessor

# Initialize processor with full configuration
processor = DataProcessor(
    base_path="data/published_data",
    output_path="data/processed",
    
    # Validation settings
    rating_min=1.0,
    rating_max=5.0,
    drop_missing_timestamps=True,  # CRITICAL: No placeholder dates
    
    # Explicit feedback thresholds
    positive_threshold=4.0,
    hard_negative_threshold=3.0,
    
    # Comment quality settings
    no_comment_quality=0.5,  # Default for missing comments
    
    # User filtering settings
    min_user_interactions=2,  # Minimum total interactions for trainable user
    min_user_positives=1,     # Must have at least 1 positive (rating ‚â•4)
    min_item_positives=5,     # Items must have ‚â•5 positive interactions
    
    # Implicit negative sampling
    implicit_negative_per_user=50,  # For evaluation
    implicit_negative_strategy='popular'  # or 'random'
)
```

#### Key Methods (Updated to Match Implementation):

**Step 1: Data Loading & Validation**
- `load_and_validate_interactions(cached_quality_scores=None)` ‚Üí `(df_clean, quality_report)`
  - Load, validate, deduplicate, detect outliers
  - **NEW**: Optional `cached_quality_scores` dict to skip re-computation
- `load_and_validate_all()` ‚Üí `Dict[str, pd.DataFrame]`
  - Load all data files (interactions, products, attributes, shops)
- `generate_quality_report(df, name)` ‚Üí `Dict`
  - Generate quality metrics report

**Step 2.0: Comment Quality & Confidence Scores**
- `compute_comment_quality(df, comment_column='processed_comment')` ‚Üí `(df_enriched, stats)`
  - AI sentiment analysis using ViSoBERT + heuristic adjustments
  - **Returns**: DataFrame with `comment_quality` [0-1] and `confidence_score` [1-6]
  - **Note**: Uses `processed_comment` column (not `comment`)

**Step 2.3: User Segmentation**
- `segment_users(interactions_df, user_col='user_id', rating_col='rating')` ‚Üí `(df_enriched, stats)`
  - Segment trainable vs cold-start users
  - **Returns**: DataFrame with `is_trainable_user` column

**Step 3: ID Mapping**
- `create_id_mappings(interactions_df, user_col='user_id', item_col='product_id')` ‚Üí `(df_mapped, mappings_dict)`
  - Create and apply bidirectional mappings
  - **Returns**: DataFrame with `u_idx`, `i_idx` columns + mappings dict

**Step 4: Temporal Split**
- `temporal_split(interactions_df, method='leave_one_out', use_validation=False)` ‚Üí `(df_with_split, split_stats)`
  - Split with temporal ordering
  - **Returns**: DataFrame with `split` column ('train'/'val'/'test')
  - **Features**: Negative holdouts, implicit negative sampling

**Step 5: Matrix Construction**
- `build_confidence_matrix(interactions_df, num_users, num_items, value_col='confidence_score')` ‚Üí `scipy.sparse.csr_matrix`
- `build_binary_matrix(...)` ‚Üí `scipy.sparse.csr_matrix`
- `build_user_positive_sets(interactions_df)` ‚Üí `Dict[int, Set[int]]`
- `build_user_hard_negative_sets(interactions_df, top_k_popular_items)` ‚Üí `Dict[int, Dict[str, Set[int]]]`
- `build_item_popularity(interactions_df, num_items, log_transform=True)` ‚Üí `np.ndarray`
- `get_top_k_popular_items(interactions_df, k=50)` ‚Üí `List[int]`
- `build_user_metadata(interactions_df)` ‚Üí `Dict`

**Step 6: Save Processed Data**
- `save_all_artifacts(...)` - Save all artifacts at once (convenience method)
- `save_interactions_parquet(interactions_df, filename='interactions.parquet')` ‚Üí `Path`
- `save_mappings_json(mappings, metadata, filename='user_item_mappings.json')` ‚Üí `Path`
- `save_csr_matrix(matrix, filename)` ‚Üí `Path`
- `save_user_sets(user_sets, filename)` ‚Üí `Path`
- `save_item_popularity(popularity, filename='item_popularity.npy')` ‚Üí `Path`
- `save_top_k_popular(top_k, filename='top_k_popular_items.json')` ‚Üí `Path`
- `save_user_metadata(metadata, filename='user_metadata.pkl')` ‚Üí `Path`
- `save_statistics_summary(stats, filename='data_stats.json')` ‚Üí `Path`

**Step 7: Data Versioning**
- `create_data_version(data_hash, filters, files, stats)` ‚Üí `version_id`
- `get_latest_data_version()` ‚Üí `Optional[Dict]`
- `compare_data_versions(version_id1, version_id2)` ‚Üí `Dict[str, Any]`
- `is_data_version_stale(version_id, max_age_hours=24)` ‚Üí `bool`

### Supporting Classes

#### `DataReader` (`processing/read_data.py`)
- **Purpose**: Load raw CSV files with proper encoding
- **Methods**:
  - `read_interactions(filepath)` ‚Üí `pd.DataFrame` - Load interactions with UTF-8
  - `read_products(filepath)` ‚Üí `pd.DataFrame` - Load product metadata
  - `read_attributes(filepath)` ‚Üí `pd.DataFrame` - Load product attributes
  - `read_shops(filepath)` ‚Üí `pd.DataFrame` - Load shop data

#### `DataAuditor` (`processing/audit_data.py`)
- **Purpose**: Validate, clean, deduplicate data
- **Methods**:
  - `validate(df)` ‚Üí `(df_valid, validation_report)` - Validate ratings, timestamps
  - `deduplicate(df, strategy='keep_latest')` ‚Üí `(df_dedup, dedup_stats)` - Remove duplicates
  - `detect_outliers(df)` ‚Üí `Dict[str, Any]` - Identify potential bots, cold items
- **Features**:
  - Strict rating range enforcement [1.0, 5.0]
  - Missing timestamp handling (drop, not impute)
  - Configurable deduplication strategy

#### `FeatureEngineer` (`processing/feature_engineering.py`)
- **AI Sentiment Model**: `5CD-AI/Vietnamese-Sentiment-visobert`
- **GPU Support**: Automatic GPU detection, batch processing (batch_size=64)
- **Methods**:
  - `compute_confidence_scores(df, comment_column='processed_comment')` ‚Üí `(df_enriched, stats)`
    - Main method combining AI sentiment + heuristics
  - `_compute_sentiment_batch(texts)` ‚Üí `np.ndarray`
    - Batch sentiment inference with ViSoBERT
  - `_apply_fake_review_checks(df, comment_column)` ‚Üí `pd.Series`
    - Heuristic quality adjustments

**Fake Review Detection Features** (NEW):
- **Length Analysis**: 
  - Bonus for long reviews (>25 words)
  - Penalty for very short reviews (<4 words)
- **Keyword Matching**: Extended Vietnamese positive/negative dictionaries
  - Positive: "th·∫•m nhanh", "hi·ªáu qu·∫£", "th∆°m", "m·ªãn", "s√°ng da", etc.
  - Negative: "k√©m", "d·ªü", "th·∫•t v·ªçng", "fake", "gi·∫£", etc.
- **Recency Decay**: Older reviews get slight down-weighting (configurable)
- **Rating-Sentiment Mismatch**: High rating + negative sentiment ‚Üí quality penalty
- **Repetition Penalty**: Low character diversity ‚Üí penalty (spam detection)
- **Emoji Sentiment Mapping** (NEW):
  - Positive emojis: üòç‚ù§Ô∏èüëç‚ú®üåüüíØüî• etc.
  - Negative emojis: üò¢üò≠üíîüëéüò° etc.
  - Neutral emojis: ü§îüòê etc.

**Usage Example**:
```python
engineer = FeatureEngineer(
    model_name="5CD-AI/Vietnamese-Sentiment-visobert",
    batch_size=64,
    no_comment_quality=0.5,
    enable_fake_review_checks=True
)

df_enriched, stats = engineer.compute_confidence_scores(
    df_clean, 
    comment_column='processed_comment'
)
# Result: df_enriched has 'comment_quality' [0-1] and 'confidence_score' [1-6]
```

#### `UserFilter` (`processing/user_filtering.py`)
- **Purpose**: Segment users into trainable vs cold-start
- **Methods**:
  - `filter_users(df, min_interactions=2, min_positives=1)` ‚Üí `(df_filtered, filter_stats)`
  - `segment_users(df)` ‚Üí `(df_with_segment, segment_stats)`
- **Features**:
  - Trainable: ‚â•2 interactions AND ‚â•1 positive (rating ‚â•4)
  - **Special Case**: 2 interactions with both negative ‚Üí force cold-start
  - Iterative item filtering after user filtering
  - Detailed statistics logging

#### `IDMapper` (`processing/id_mapping.py`)
- **Purpose**: Create contiguous ID mappings
- **Methods**:
  - `create_mappings(df, user_col, item_col)` ‚Üí `Dict[str, Dict]`
  - `apply_mappings(df, mappings)` ‚Üí `pd.DataFrame`
  - `get_reverse_mapping(mapping)` ‚Üí `Dict[int, int]`
- **Output Structure**:
  ```python
  {
      "user_to_idx": {original_id: idx, ...},
      "idx_to_user": {idx: original_id, ...},
      "item_to_idx": {original_id: idx, ...},
      "idx_to_item": {idx: original_id, ...}
  }
  ```

#### `TemporalSplitter` (`processing/temporal_split.py`)
- **Purpose**: Leave-one-out temporal split with positive-only test
- **Methods**:
  - `split(df, method='leave_one_out', use_validation=False)` ‚Üí `(df_split, split_stats)`
  - `_sample_implicit_negatives(df, k=50, strategy='popular')` ‚Üí `Dict[int, Set[int]]`
- **Features** (UPDATED - Optimized Implementation):
  - **Vectorized Operations**: 10-100x speedup vs iterative approach
    - Uses `groupby().transform()` for efficient per-user operations
    - Avoids Python loops for better performance
  - **Implicit Negative Sampling**: 
    - Strategy: 'popular' (Top-K popular items) or 'random'
    - Default: 50 items per user for unbiased evaluation
  - **Negative Holdouts**: Reserve explicit negatives (rating ‚â§3) for analysis
  - **Edge Case Handling**:
    - Users with exactly 2 interactions (both positive/negative scenarios)
    - Users with no positives ‚Üí excluded from test
    - Latest interaction negative ‚Üí find previous positive for test

**Performance Optimization**:
```python
# OLD (slow iterative approach):
# for u_idx in df['u_idx'].unique():
#     user_df = df[df['u_idx'] == u_idx]
#     ...

# NEW (fast vectorized approach):
# Rank interactions by timestamp within each user group
df['rank_desc'] = df.groupby('u_idx')['cmt_date'].rank(
    method='first', ascending=False
)
# Identify latest positive per user
df['is_latest_positive'] = (
    df.groupby('u_idx').apply(
        lambda g: g[g['is_positive']]['cmt_date'].idxmax()
    ).values == df.index
)
```

#### `MatrixBuilder` (`processing/matrix_construction.py`)
- **Purpose**: Build sparse matrices and auxiliary data structures
- **Methods**:
  - `build_confidence_matrix(df, num_users, num_items, value_col)` ‚Üí `csr_matrix`
  - `build_binary_matrix(df, num_users, num_items)` ‚Üí `csr_matrix`
  - `build_user_positive_sets(df, user_col, item_col)` ‚Üí `Dict[int, Set[int]]`
  - `build_user_hard_negative_sets(df, top_k_popular, ...)` ‚Üí `Dict[int, Dict[str, Set[int]]]`
    - Returns: `{u_idx: {"explicit": Set[i_idx], "implicit": Set[i_idx]}}`
  - `build_item_popularity(df, num_items, log_transform=True)` ‚Üí `np.ndarray`
  - `get_top_k_popular_items(df, k=50)` ‚Üí `List[int]`
  - `build_user_metadata(df, trainable_col='is_trainable_user')` ‚Üí `Dict`

**Usage Example**:
```python
builder = MatrixBuilder()

# Build confidence matrix for ALS
X_confidence = builder.build_confidence_matrix(
    df_train, num_users=26000, num_items=2244,
    value_col='confidence_score'
)  # Shape: (26000, 2244), values: [1.0, 6.0]

# Build hard negative sets (explicit + implicit)
top_k = builder.get_top_k_popular_items(df_train, k=50)
hard_negs = builder.build_user_hard_negative_sets(
    df_train, top_k, 
    user_col='u_idx', item_col='i_idx',
    rating_col='rating', threshold=3.0
)
# Result: {u_idx: {"explicit": {items rated ‚â§3}, "implicit": {popular uninteracted}}}
```

#### `DataSaver` (`processing/data_saver.py`)
- **Purpose**: Save all artifacts in appropriate formats
- **Methods**:
  - `save_interactions_parquet(df, filepath)` ‚Üí `Path`
  - `save_mappings_json(mappings, metadata, filepath)` ‚Üí `Path`
  - `save_csr_matrix(matrix, filepath)` ‚Üí `Path` (NPZ format)
  - `save_user_sets(sets, filepath)` ‚Üí `Path` (Pickle format)
  - `save_item_popularity(array, filepath)` ‚Üí `Path` (NumPy format)
  - `save_top_k_popular(items, filepath)` ‚Üí `Path` (JSON format)
  - `save_user_metadata(metadata, filepath)` ‚Üí `Path` (Pickle format)
  - `save_statistics_summary(stats, filepath)` ‚Üí `Path` (JSON format)
  - `save_all_artifacts(...)` ‚Üí `Dict[str, Path]` (convenience method)

**Artifact Formats**:
- **Parquet**: Interactions data (10x faster, 50% size reduction)
- **JSON**: Mappings, top-k items, statistics (human-readable)
- **NPZ**: Sparse matrices (scipy CSR format)
- **Pickle**: User sets, metadata (Python objects)
- **NumPy**: Popularity array

#### `VersionRegistry` (`processing/version_registry.py`)
- **Purpose**: Track data versions for reproducibility
- **Methods**:
  - `create_version(data_hash, filters, files, stats)` ‚Üí `version_id`
  - `get_version(version_id)` ‚Üí `Optional[Dict]`
  - `get_latest_version()` ‚Üí `Optional[Dict]`
  - `compare_versions(v1, v2)` ‚Üí `Dict[str, Any]`
    - Compare two versions (file changes, stat diffs)
  - `is_stale(version_id, max_age_hours=24)` ‚Üí `bool`
    - Check if version is outdated
  - `find_version_by_hash(data_hash)` ‚Üí `Optional[Dict]`
    - Find version matching specific data hash

**Version Entry Structure**:
```json
{
  "v20250115_103000": {
    "version_id": "v20250115_103000",
    "data_hash": "abc123...",
    "git_commit": "def456...",
    "created_at": "2025-01-15T10:30:00",
    "filters": {
      "min_user_interactions": 2,
      "min_user_positives": 1,
      "min_item_positives": 5
    },
    "files": ["interactions.parquet", "user_item_mappings.json", ...],
    "stats": {
      "num_users": 26000,
      "num_items": 2200,
      "num_interactions": 65000,
      "matrix_density": 0.0011
    }
  }
}

### Backward Compatibility

Module v·∫´n support c√°c convenience functions cho backward compatibility:

```python
from recsys.cf.data import (
    # Main class
    DataProcessor,
    
    # Processing classes (for advanced usage)
    DataReader,
    DataAuditor,
    FeatureEngineer,
    UserFilter,
    IDMapper,
    TemporalSplitter,
    MatrixBuilder,
    DataSaver,
    VersionRegistry,
    
    # Convenience functions (backward compatible)
    load_raw_data,
    validate_and_clean_interactions,
    deduplicate_interactions,
    detect_outliers,
    compute_data_hash,
    log_data_quality_report
)

# Old style still works
data = load_raw_data("data/published_data")
df_clean, stats = validate_and_clean_interactions(data['interactions'])

# New recommended style
processor = DataProcessor(base_path="data/published_data")
df_clean, quality_report = processor.load_and_validate_interactions()
```

### Complete Pipeline Example

```python
from recsys.cf.data import DataProcessor

# Initialize processor
processor = DataProcessor(
    base_path="data/published_data",
    output_path="data/processed",
    positive_threshold=4.0,
    hard_negative_threshold=3.0,
    no_comment_quality=0.5
)

# Step 1: Load & Validate
df_clean, quality_report = processor.load_and_validate_interactions()
print(f"Loaded {len(df_clean)} interactions, {quality_report['valid_rows']} valid")

# Step 2.0: Compute Comment Quality (AI Sentiment)
df_enriched, quality_stats = processor.compute_comment_quality(
    df_clean, comment_column='processed_comment'
)
print(f"Mean comment_quality: {quality_stats['mean_quality']:.3f}")

# Step 2.3: Segment Users
df_segmented, segment_stats = processor.segment_users(df_enriched)
print(f"Trainable users: {segment_stats['trainable_count']} ({segment_stats['trainable_pct']:.1f}%)")

# Step 3: ID Mapping
df_mapped, mappings = processor.create_id_mappings(df_segmented)
num_users = len(mappings['user_to_idx'])
num_items = len(mappings['item_to_idx'])

# Step 4: Temporal Split
df_split, split_stats = processor.temporal_split(df_mapped, method='leave_one_out')
print(f"Train: {split_stats['train_size']}, Test: {split_stats['test_size']}")

# Step 5: Build Matrices
df_train = df_split[df_split['split'] == 'train']
X_confidence = processor.build_confidence_matrix(df_train, num_users, num_items)
user_pos_sets = processor.build_user_positive_sets(df_train)
top_k_popular = processor.get_top_k_popular_items(df_train, k=50)
user_hard_neg_sets = processor.build_user_hard_negative_sets(df_train, top_k_popular)
user_metadata = processor.build_user_metadata(df_split)

# Step 6: Save All Artifacts
saved_files = processor.save_all_artifacts(
    interactions_df=df_split,
    mappings=mappings,
    X_confidence=X_confidence,
    user_pos_sets=user_pos_sets,
    user_hard_neg_sets=user_hard_neg_sets,
    user_metadata=user_metadata,
    top_k_popular=top_k_popular,
    stats=split_stats
)
print(f"Saved {len(saved_files)} artifacts")

# Step 7: Create Version
data_hash = processor.compute_data_hash()
version_id = processor.create_data_version(
    data_hash=data_hash,
    filters={'min_user_interactions': 2, 'min_user_positives': 1},
    files=list(saved_files.keys()),
    stats=split_stats
)
print(f"Created version: {version_id}")
```

### Content Enrichment (Separate Module)

**Note**: Content enrichment (PhoBERT embeddings, metadata standardization) is handled in separate modules:
- `recsys/content/metadata.py` - Metadata standardization
- `recsys/bert/embedding_generator.py` - PhoBERT embedding generation

See **Component 8: BERT/PhoBERT Embeddings Pipeline** below for details.

## Component 8: BERT/PhoBERT Embeddings Pipeline

### Purpose
T√≠ch h·ª£p BERT embeddings v√†o data layer ƒë·ªÉ h·ªó tr·ª£ hybrid reranking v√† content-based fallback.

### Step 0: Metadata Standardization & Auxiliary Signals

#### 0.1 Standardize skin_type for Hard Filtering
```python
def standardize_skin_type(raw_text):
    """
    Chu·∫©n h√≥a skin_type t·ª´ text t·ª± do sang danh s√°ch chu·∫©n.
    
    Input: "Da m·ª•n tr·ª©ng c√°, Da h·ªón h·ª£p..."
    Output: ['acne', 'combination']
    """
    skin_type_mapping = {
        'm·ª•n': 'acne',
        'tr·ª©ng c√°': 'acne',
        'h·ªón h·ª£p': 'combination',
        'd·∫ßu': 'oily',
        'kh√¥': 'dry',
        'nh·∫°y c·∫£m': 'sensitive',
        'th∆∞·ªùng': 'normal',
        'm·ªçi lo·∫°i': 'all'
    }
    
    if pd.isna(raw_text):
        return ['all']
    
    raw_lower = raw_text.lower()
    detected_types = []
    
    for keyword, standard_type in skin_type_mapping.items():
        if keyword in raw_lower:
            detected_types.append(standard_type)
    
    return detected_types if detected_types else ['all']

# Apply standardization
attributes_df['skin_type_standardized'] = attributes_df['skin_type'].apply(standardize_skin_type)
```

#### 0.2 Prepare Auxiliary Signals for Reranking
```python
# Popularity signal v·ªõi log-transform
attributes_df['popularity_score'] = np.log1p(attributes_df['num_sold_time'].fillna(0))

# Quality signal
# Option 1: T·ª´ attribute file
attributes_df['quality_score'] = attributes_df['is_5_star'].fillna(0)

# Option 2: T√≠nh t·ª´ review data
product_quality = reviews_df.groupby('product_id')['rating'].agg([
    ('avg_rating', 'mean'),
    ('num_ratings', 'count'),
    ('pct_5star', lambda x: (x == 5).sum() / len(x))
]).reset_index()

# Merge back
attributes_df = attributes_df.merge(product_quality, on='product_id', how='left')

# Save enriched attributes
attributes_df.to_parquet('data/processed/product_attributes_enriched.parquet')
```

**Output Artifacts**:
- `data/processed/product_attributes_enriched.parquet` v·ªõi columns:
  - `product_id`, `ingredient`, `feature`, `skin_type_standardized`
  - `popularity_score` (log-transformed)
  - `quality_score`, `avg_rating`, `pct_5star`
  - `price`, `brand`, `origin`, etc.

### Step 1: Extract Product Descriptions

#### Load Rich Product Text Data
```python
# Load product metadata
products_df = pd.read_csv('data/published_data/data_product.csv', encoding='utf-8')
attributes_df = pd.read_csv('data/published_data/attribute_based_embeddings/attribute_text_filtering.csv', encoding='utf-8')

# Merge ƒë·ªÉ c√≥ full context
products_enriched = products_df.merge(attributes_df, on='product_id', how='left')

# Create "Super Text" for PhoBERT v·ªõi Vietnamese context
products_enriched['bert_input_text'] = (
    'T√™n: ' + products_enriched['product_name'] + ' [SEP] ' +
    'Th√†nh ph·∫ßn: ' + products_enriched['ingredient'].fillna('Kh√¥ng r√µ') + ' [SEP] ' +
    'C√¥ng d·ª•ng: ' + products_enriched['feature'].fillna('Kh√¥ng r√µ') + ' [SEP] ' +
    'Lo·∫°i da ph√π h·ª£p: ' + products_enriched['skin_type'].fillna('M·ªçi lo·∫°i da') + ' [SEP] ' +
    'Th∆∞∆°ng hi·ªáu: ' + products_enriched['brand'].fillna('') + ' [SEP] ' +
    'M√¥ t·∫£: ' + products_enriched['processed_description'].fillna('')
)
```

**Rationale**: 
- N·ªëi nhi·ªÅu tr∆∞·ªùng metadata t·∫°o ng·ªØ c·∫£nh phong ph√∫
- PhoBERT s·∫Ω h·ªçc ƒë∆∞·ª£c semantic similarity s√¢u (v√≠ d·ª•: s·∫£n ph·∫©m kh√°c brand nh∆∞ng c√πng BHA + Da d·∫ßu ‚Üí vector g·∫ßn nhau)
- Token `[SEP]` gi√∫p model ph√¢n bi·ªát c√°c tr∆∞·ªùng th√¥ng tin

### Step 2: Generate BERT Embeddings

#### Module: `recsys/bert/embedding_generator.py`

```python
import torch
from transformers import AutoTokenizer, AutoModel
import numpy as np
from tqdm import tqdm

class BERTEmbeddingGenerator:
    """
    Generate BERT/PhoBERT embeddings cho product descriptions.
    """
    
    def __init__(self, model_name='vinai/phobert-base', device='cpu'):
        self.device = device
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name).to(device)
        self.model.eval()
    
    def encode_texts(self, texts, batch_size=32, max_length=256):
        """
        Encode texts th√†nh embeddings.
        
        Args:
            texts: List of text strings
            batch_size: Batch size cho encoding
            max_length: Max token length
        
        Returns:
            np.array: (len(texts), hidden_dim) embeddings
        """
        embeddings = []
        
        with torch.no_grad():
            for i in tqdm(range(0, len(texts), batch_size)):
                batch_texts = texts[i:i+batch_size]
                
                # Tokenize
                encoded = self.tokenizer(
                    batch_texts,
                    padding=True,
                    truncation=True,
                    max_length=max_length,
                    return_tensors='pt'
                ).to(self.device)
                
                # Forward pass
                outputs = self.model(**encoded)
                
                # Mean pooling over sequence
                batch_embeddings = outputs.last_hidden_state.mean(dim=1)
                embeddings.append(batch_embeddings.cpu().numpy())
        
        return np.vstack(embeddings)
    
    def save_embeddings(self, embeddings, product_ids, output_path):
        """
        Save embeddings v·ªõi metadata.
        
        Args:
            embeddings: np.array (N, D)
            product_ids: List of product IDs
            output_path: Path to save .pt file
        """
        torch.save({
            'embeddings': torch.from_numpy(embeddings),
            'product_ids': product_ids,
            'model_name': self.tokenizer.name_or_path,
            'embedding_dim': embeddings.shape[1],
            'num_products': len(product_ids),
            'created_at': datetime.now().isoformat()
        }, output_path)
```

### Step 3: Embedding Generation Workflow

#### Script: `scripts/generate_bert_embeddings.py`
```python
"""
Generate BERT embeddings cho all products v·ªõi rich metadata.

Usage:
    python scripts/generate_bert_embeddings.py \
        --model vinai/phobert-base \
        --output data/processed/content_based_embeddings/
"""

import argparse
import pandas as pd
from recsys.bert.embedding_generator import BERTEmbeddingGenerator

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', default='vinai/phobert-base')
    parser.add_argument('--output', default='data/processed/content_based_embeddings/')
    parser.add_argument('--batch-size', type=int, default=32)
    args = parser.parse_args()
    
    # Load products with rich metadata
    products = pd.read_csv('data/published_data/data_product.csv', encoding='utf-8')
    attributes = pd.read_csv('data/published_data/attribute_based_embeddings/attribute_text_filtering.csv', encoding='utf-8')
    
    # Merge and create super text
    products_enriched = products.merge(attributes, on='product_id', how='left')
    products_enriched['bert_input_text'] = (
        'T√™n: ' + products_enriched['product_name'] + ' [SEP] ' +
        'Th√†nh ph·∫ßn: ' + products_enriched['ingredient'].fillna('Kh√¥ng r√µ') + ' [SEP] ' +
        'C√¥ng d·ª•ng: ' + products_enriched['feature'].fillna('Kh√¥ng r√µ') + ' [SEP] ' +
        'Lo·∫°i da ph√π h·ª£p: ' + products_enriched['skin_type'].fillna('M·ªçi lo·∫°i da') + ' [SEP] ' +
        'Th∆∞∆°ng hi·ªáu: ' + products_enriched['brand'].fillna('') + ' [SEP] ' +
        'M√¥ t·∫£: ' + products_enriched['processed_description'].fillna('')
    )
    
    # Generate embeddings
    generator = BERTEmbeddingGenerator(model_name=args.model)
    embeddings = generator.encode_texts(
        products_enriched['bert_input_text'].tolist(),
        batch_size=args.batch_size
    )
    
    # Save
    os.makedirs(args.output, exist_ok=True)
    output_file = os.path.join(args.output, 'product_embeddings.pt')
    generator.save_embeddings(
        embeddings,
        products_enriched['product_id'].tolist(),
        output_file
    )
    
    print(f"Saved {len(embeddings)} embeddings to {output_file}")
    print(f"Embedding dimension: {embeddings.shape[1]}")

if __name__ == '__main__':
    main()
```

### Step 4: User Profile Embeddings

#### Strategy 1: Interaction-Weighted Average
```python
def compute_user_profile_embedding(user_history_items, item_embeddings, item_to_idx):
    """
    Compute user profile b·∫±ng weighted average c·ªßa item embeddings.
    
    Args:
        user_history_items: List[(product_id, weight)]
        item_embeddings: np.array (num_items, dim)
        item_to_idx: Dict mapping product_id -> idx
    
    Returns:
        np.array: (dim,) user profile embedding
    """
    history_embeddings = []
    weights = []
    
    for product_id, weight in user_history_items:
        if product_id in item_to_idx:
            idx = item_to_idx[product_id]
            history_embeddings.append(item_embeddings[idx])
            weights.append(weight)
    
    if not history_embeddings:
        # Fallback: zero embedding ho·∫∑c mean embedding
        return np.zeros(item_embeddings.shape[1])
    
    # Weighted average
    history_embeddings = np.array(history_embeddings)
    weights = np.array(weights).reshape(-1, 1)
    weights = weights / weights.sum()  # Normalize
    
    user_profile = (history_embeddings * weights).sum(axis=0)
    return user_profile
```

#### Strategy 2: TF-IDF Weighted
```python
from sklearn.feature_extraction.text import TfidfVectorizer

def compute_user_tfidf_profile(user_history_texts, item_embeddings, product_ids):
    """
    Compute user profile b·∫±ng TF-IDF weighted item embeddings.
    """
    # Compute TF-IDF weights
    vectorizer = TfidfVectorizer(max_features=1000)
    tfidf_matrix = vectorizer.fit_transform(user_history_texts)
    
    # Weight embeddings by TF-IDF importance
    # ... (implementation)
```

### Step 5: Embedding Versioning

#### Metadata File: `data/processed/content_based_embeddings/embedding_metadata.json`
```json
{
  "version": "v1_20250115_103000",
  "model_name": "vinai/phobert-base",
  "embedding_dim": 768,
  "num_products": 2244,
  "created_at": "2025-01-15T10:30:00",
  "data_hash": "abc123...",
  "git_commit": "def456...",
  "files": {
    "product_embeddings": "product_embeddings.pt",
    "user_profiles": "user_profile_embeddings.pt"
  }
}
```

### Step 6: Sync with CF Data

#### Validation: Check Alignment
```python
def validate_embedding_alignment(mappings_path, embeddings_path):
    """
    Validate BERT embeddings align v·ªõi CF item mappings.
    """
    # Load mappings
    with open(mappings_path) as f:
        mappings = json.load(f)
    
    # Load embeddings
    embeddings_data = torch.load(embeddings_path)
    
    cf_product_ids = set(mappings['item_to_idx'].keys())
    bert_product_ids = set(str(pid) for pid in embeddings_data['product_ids'])
    
    # Check coverage
    missing_in_bert = cf_product_ids - bert_product_ids
    extra_in_bert = bert_product_ids - cf_product_ids
    
    print(f"CF products: {len(cf_product_ids)}")
    print(f"BERT products: {len(bert_product_ids)}")
    print(f"Missing in BERT: {len(missing_in_bert)}")
    print(f"Extra in BERT: {len(extra_in_bert)}")
    
    if missing_in_bert:
        warnings.warn(f"Warning: {len(missing_in_bert)} products have CF embeddings but no BERT embeddings")
```

## Dependencies

```python
# requirements_data.txt
pandas>=1.5.0
numpy>=1.23.0
scipy>=1.9.0
pyarrow>=10.0.0  # For parquet

# BERT dependencies
torch>=1.13.0
transformers>=4.25.0
sentencepiece>=0.1.96  # For PhoBERT tokenizer
```

## Error Handling

### Common Errors
1. **CSV encoding error** ‚Üí Enforce UTF-8, log problematic rows
2. **Missing columns** ‚Üí Raise clear error v·ªõi expected schema
3. **Type conversion failure** ‚Üí Log rows, fill with defaults ho·∫∑c drop
4. **Mapping collision** ‚Üí Should not happen v·ªõi unique IDs, validate
5. **Empty DataFrame after filtering** ‚Üí Log warning, adjust thresholds

### Logging Strategy
- **Level INFO**: Summary stats (rows processed, filtered)
- **Level WARNING**: Outliers, missing data
- **Level ERROR**: Critical failures (corrupt file, schema mismatch)
- **Output**: `logs/data_processing.log` v·ªõi rotation

## Monitoring Metrics

### Data Quality Metrics
- **Sparsity**: % of nonzero cells trong matrix
- **Coverage**: % users/items c√≤n l·∫°i sau filtering
- **Rating distribution**: Mean, std, quantiles
- **Temporal spread**: Min/max timestamps, gaps

### Drift Detection (For Retraining)
- **Distribution shift**: KL divergence c·ªßa rating distribution
- **Popularity shift**: Spearman correlation c·ªßa item ranks
- **User growth**: % new users vs existing
- **Trigger**: Retrain n·∫øu shift > threshold

## Timeline & Status

### Implementation Status: ‚úÖ COMPLETED (January 2025)

| Phase | Estimated | Actual | Status |
|-------|-----------|--------|--------|
| Core Classes Implementation | 2-3 days | ~3 days | ‚úÖ Done |
| AI Sentiment Integration | 1 day | ~1 day | ‚úÖ Done |
| Vectorized Optimization | 0.5 day | ~0.5 day | ‚úÖ Done |
| Testing & Validation | 1 day | ~1 day | ‚úÖ Done |
| Documentation | 0.5 day | ~0.5 day | ‚úÖ Done |
| **Total** | ~4-5 days | ~6 days | ‚úÖ Complete |

### Modules Implemented:
- `DataProcessor` (main orchestrator): ~1000+ lines
- `DataReader`: CSV loading with UTF-8
- `DataAuditor`: Validation, deduplication, outlier detection
- `FeatureEngineer`: ViSoBERT sentiment, fake review detection, GPU batch processing
- `UserFilter`: Trainable/cold-start segmentation
- `IDMapper`: Bidirectional ID mappings
- `TemporalSplitter`: Optimized vectorized split (10-100x faster)
- `MatrixBuilder`: CSR matrices, user sets, metadata
- `DataSaver`: All artifact formats (Parquet, JSON, NPZ, Pickle)
- `VersionRegistry`: Version tracking, comparison, staleness detection

### Next Steps:
- Task 02: ALS/BPR Training (uses X_train_confidence.npz)
- Task 03: Model Evaluation (uses test split + implicit negatives)
- Task 05: Serving Layer (uses user_metadata.pkl for routing)

## Success Criteria

### Core Functionality ‚úÖ
- [x] Pipeline ch·∫°y end-to-end kh√¥ng errors
- [x] Output artifacts pass all quality checks (11 files)
- [x] Processing time <15 minutes cho 369K interactions
- [x] Reproducible: Same input ‚Üí same output (v·ªõi data_hash tracking)
- [x] Documented: Clear README v√† inline comments

### Data Validation ‚úÖ
- [x] No NaT timestamps trong processed data (strict enforcement)
- [x] All ratings trong range [1.0, 5.0] (validated and enforced)
- [x] Test set ch·ªâ ch·ª©a positive interactions (rating ‚â•4)
- [x] Confidence matrix (X_train_confidence) c√≥ values trong [1.0, 6.0]

### AI/ML Features ‚úÖ
- [x] AI sentiment model (ViSoBERT) successfully processes all comments
- [x] Comment quality scores computed for all interactions (including missing ‚Üí default 0.5)
- [x] Fake review detection heuristics applied (length, keywords, emoji, mismatch)
- [x] GPU batch processing enabled (batch_size=64)

### User Segmentation ‚úÖ
- [x] Trainable users correctly identified (‚â•2 interactions, ‚â•1 positive)
- [x] Cold-start users flagged with `is_trainable_user = False`
- [x] Special case: 2 interactions both negative ‚Üí force cold-start

### Matrix Construction ‚úÖ
- [x] CSR matrices shape matches (num_trainable_users, num_items)
- [x] user_pos_train keys = all u_idx in train v·ªõi positives
- [x] user_hard_neg_train contains both explicit and implicit negatives
- [x] Hard negative sets coverage ‚â•50% of trainable users

### Temporal Split ‚úÖ
- [x] No data leakage: Test timestamps > Train timestamps per user
- [x] Vectorized implementation (10-100x faster)
- [x] Implicit negatives sampled (50 per user) for evaluation
- [x] Edge cases handled (2 interactions, all-negative users)

### Content Enrichment (Separate Module)
- [ ] PhoBERT embeddings coverage 100% products
- [ ] skin_type_standardized ch·ª©a valid list values
- [ ] popularity_score v√† quality_score kh√¥ng c√≥ NaN

### Versioning ‚úÖ
- [x] All artifacts embed data_hash v√† git_commit
- [x] Version comparison v√† staleness detection working
- [x] versions.json properly maintained
