# HÆ°á»›ng Dáº«n Xá»­ LÃ½ Dá»¯ Liá»‡u (Data Processing Guide)

> **Module**: `recsys/cf/data/`  
> **Version**: 1.0 (January 2025)  
> **Status**: âœ… Production Ready

## Má»¥c Lá»¥c

1. [Tá»•ng Quan](#1-tá»•ng-quan)
2. [Kiáº¿n TrÃºc Module](#2-kiáº¿n-trÃºc-module)
3. [CÃ i Äáº·t & YÃªu Cáº§u](#3-cÃ i-Ä‘áº·t--yÃªu-cáº§u)
4. [HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng](#4-hÆ°á»›ng-dáº«n-sá»­-dá»¥ng)
5. [Chi Tiáº¿t CÃ¡c BÆ°á»›c Xá»­ LÃ½](#5-chi-tiáº¿t-cÃ¡c-bÆ°á»›c-xá»­-lÃ½)
6. [Output Artifacts](#6-output-artifacts)
7. [Cáº¥u HÃ¬nh NÃ¢ng Cao](#7-cáº¥u-hÃ¬nh-nÃ¢ng-cao)
8. [Xá»­ LÃ½ Lá»—i & Debug](#8-xá»­-lÃ½-lá»—i--debug)
9. [FAQ](#9-faq)

---

## 1. Tá»•ng Quan

### 1.1 Data Processing Pipeline lÃ  gÃ¬?

Data Processing Pipeline lÃ  bá»™ cÃ´ng cá»¥ xá»­ lÃ½ dá»¯ liá»‡u thÃ´ (raw data) tá»« cÃ¡c file CSV thÃ nh cÃ¡c Ä‘á»‹nh dáº¡ng tá»‘i Æ°u cho viá»‡c training mÃ´ hÃ¬nh Collaborative Filtering (ALS, BPR). Pipeline nÃ y giáº£i quyáº¿t cÃ¡c thÃ¡ch thá»©c Ä‘áº·c thÃ¹ cá»§a dataset:

| ThÃ¡ch Thá»©c | Giáº£i PhÃ¡p |
|------------|-----------|
| **Sparse Data** (~1.23 interactions/user) | PhÃ¢n loáº¡i user thÃ nh trainable vs cold-start |
| **Rating Skew** (95% lÃ  5 sao) | AI Sentiment Analysis táº¡o confidence scores |
| **Vietnamese Text** | ViSoBERT model cho sentiment tiáº¿ng Viá»‡t |
| **Large Scale** (369K interactions) | Vectorized operations, GPU batch processing |

### 1.2 Input & Output

```
ğŸ“ INPUT (data/published_data/)
â”œâ”€â”€ data_reviews_purchase.csv    # 369K reviews
â”œâ”€â”€ data_product.csv             # 2,244 products
â”œâ”€â”€ data_product_attribute.csv   # Product attributes
â””â”€â”€ data_shop.csv               # Shop metadata

        â¬‡ï¸ Data Processing Pipeline â¬‡ï¸

ğŸ“ OUTPUT (data/processed/)
â”œâ”€â”€ interactions.parquet         # Processed interactions
â”œâ”€â”€ user_item_mappings.json      # ID mappings
â”œâ”€â”€ X_train_confidence.npz       # ALS training matrix
â”œâ”€â”€ user_pos_train.pkl           # Positive item sets
â”œâ”€â”€ user_hard_neg_train.pkl      # Hard negative sets
â”œâ”€â”€ user_metadata.pkl            # User segmentation
â”œâ”€â”€ item_popularity.npy          # Popularity scores
â”œâ”€â”€ top_k_popular_items.json     # Top-50 popular items
â”œâ”€â”€ data_stats.json              # Statistics
â””â”€â”€ versions.json                # Version tracking
```

---

## 2. Kiáº¿n TrÃºc Module

### 2.1 Class Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       DataProcessor                              â”‚
â”‚  (Main Orchestrator - recsys/cf/data/data.py)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  + load_and_validate_interactions()                             â”‚
â”‚  + compute_comment_quality()                                     â”‚
â”‚  + segment_users()                                               â”‚
â”‚  + create_id_mappings()                                          â”‚
â”‚  + temporal_split()                                              â”‚
â”‚  + build_confidence_matrix()                                     â”‚
â”‚  + save_all_artifacts()                                          â”‚
â”‚  + create_data_version()                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚                     â”‚
        â–¼                     â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DataReader   â”‚   â”‚  DataAuditor  â”‚   â”‚FeatureEngineerâ”‚
â”‚  (read_data)  â”‚   â”‚ (audit_data)  â”‚   â”‚(feature_eng)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ read_csv()    â”‚   â”‚ validate()    â”‚   â”‚ compute_      â”‚
â”‚ UTF-8 support â”‚   â”‚ deduplicate() â”‚   â”‚ sentiment()   â”‚
â”‚               â”‚   â”‚ detect_       â”‚   â”‚ fake_review   â”‚
â”‚               â”‚   â”‚ outliers()    â”‚   â”‚ detection()   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                     â”‚                     â”‚
        â–¼                     â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  UserFilter   â”‚   â”‚   IDMapper    â”‚   â”‚TemporalSplit  â”‚
â”‚(user_filter)  â”‚   â”‚ (id_mapping)  â”‚   â”‚(temporal_splitâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ segment_      â”‚   â”‚ create_       â”‚   â”‚ leave_one_out â”‚
â”‚ users()       â”‚   â”‚ mappings()    â”‚   â”‚ vectorized    â”‚
â”‚ trainable vs  â”‚   â”‚ bidirectional â”‚   â”‚ implicit_neg  â”‚
â”‚ cold-start    â”‚   â”‚               â”‚   â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                     â”‚                     â”‚
        â–¼                     â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MatrixBuilder â”‚   â”‚  DataSaver    â”‚   â”‚VersionRegistryâ”‚
â”‚(matrix_const) â”‚   â”‚ (data_saver)  â”‚   â”‚(version_reg)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ build_csr()   â”‚   â”‚ save_parquet()â”‚   â”‚ create_       â”‚
â”‚ user_sets()   â”‚   â”‚ save_json()   â”‚   â”‚ version()     â”‚
â”‚ hard_negs()   â”‚   â”‚ save_npz()    â”‚   â”‚ compare()     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 7-Step Pipeline Flow

```
Step 1: Load & Validate    â”€â”€â–º Step 2: Feature Engineering â”€â”€â–º Step 3: ID Mapping
   â”‚                              â”‚                               â”‚
   â”œâ”€ Read CSV (UTF-8)           â”œâ”€ AI Sentiment (ViSoBERT)      â”œâ”€ user_to_idx
   â”œâ”€ Validate ratings           â”œâ”€ Fake review detection        â”œâ”€ item_to_idx
   â”œâ”€ Drop missing timestamps    â”œâ”€ Emoji sentiment              â””â”€ Contiguous IDs
   â””â”€ Deduplicate                â””â”€ confidence_score [1-6]
                                          â”‚
                                          â–¼
Step 4: Temporal Split    â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ User Segmentation
   â”‚                                      â”‚
   â”œâ”€ Leave-one-out                      â”œâ”€ Trainable: â‰¥2 interactions
   â”œâ”€ Positive-only test                 â”œâ”€ Cold-start: <2 interactions
   â”œâ”€ Vectorized (10-100x faster)        â””â”€ is_trainable_user flag
   â””â”€ Implicit negatives (50/user)
        â”‚
        â–¼
Step 5: Matrix Construction â”€â”€â–º Step 6: Save Artifacts â”€â”€â–º Step 7: Versioning
   â”‚                               â”‚                          â”‚
   â”œâ”€ X_train_confidence.npz      â”œâ”€ Parquet                  â”œâ”€ data_hash
   â”œâ”€ user_pos_train.pkl          â”œâ”€ JSON                     â”œâ”€ git_commit
   â”œâ”€ user_hard_neg_train.pkl     â”œâ”€ NPZ (sparse)             â””â”€ versions.json
   â””â”€ item_popularity.npy         â””â”€ Pickle
```

---

## 3. CÃ i Äáº·t & YÃªu Cáº§u

### 3.1 Dependencies

```bash
# Core dependencies
pip install pandas>=1.5.0 numpy>=1.23.0 scipy>=1.9.0 pyarrow>=10.0.0

# AI Sentiment (ViSoBERT)
pip install torch>=1.13.0 transformers>=4.25.0 sentencepiece>=0.1.96

# Optional: GPU acceleration
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

### 3.2 Hardware Requirements

| Component | Minimum | Recommended |
|-----------|---------|-------------|
| RAM | 8 GB | 16 GB |
| GPU | None (CPU fallback) | NVIDIA 8GB+ VRAM |
| Storage | 5 GB | 10 GB |
| CPU | 4 cores | 8+ cores |

### 3.3 Verify Installation

```python
# Kiá»ƒm tra installation
from recsys.cf.data import DataProcessor

# Kiá»ƒm tra GPU (optional)
import torch
print(f"GPU available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
```

---

## 4. HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng

### 4.1 Quick Start (5 phÃºt)

```python
from recsys.cf.data import DataProcessor

# Khá»Ÿi táº¡o processor vá»›i config máº·c Ä‘á»‹nh
processor = DataProcessor(
    base_path="data/published_data",
    output_path="data/processed"
)

# Cháº¡y pipeline Ä‘áº§y Ä‘á»§
df_clean, _ = processor.load_and_validate_interactions()
df_enriched, _ = processor.compute_comment_quality(df_clean)
df_segmented, _ = processor.segment_users(df_enriched)
df_mapped, mappings = processor.create_id_mappings(df_segmented)
df_split, stats = processor.temporal_split(df_mapped)

# Build matrices vÃ  save
# ... (xem Full Pipeline Example bÃªn dÆ°á»›i)
```

### 4.2 Full Pipeline Example

```python
from recsys.cf.data import DataProcessor
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# KHá»I Táº O PROCESSOR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
processor = DataProcessor(
    # Paths
    base_path="data/published_data",
    output_path="data/processed",
    
    # Thresholds
    positive_threshold=4.0,      # rating >= 4 â†’ positive
    hard_negative_threshold=3.0, # rating <= 3 â†’ hard negative
    
    # Comment quality
    no_comment_quality=0.5,      # Default cho missing comments
    
    # User filtering
    min_user_interactions=2,     # Minimum Ä‘á»ƒ trainable
    min_user_positives=1,        # Ãt nháº¥t 1 positive
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 1: LOAD & VALIDATE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
logger.info("Step 1: Loading and validating data...")
df_clean, quality_report = processor.load_and_validate_interactions()

print(f"""
ğŸ“Š Data Quality Report:
   - Total rows: {quality_report['total_rows']:,}
   - Valid rows: {quality_report['valid_rows']:,}
   - Duplicates removed: {quality_report.get('duplicates_removed', 0):,}
   - Invalid ratings: {quality_report.get('invalid_ratings', 0):,}
""")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 2: FEATURE ENGINEERING (AI Sentiment)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
logger.info("Step 2: Computing comment quality with AI sentiment...")
df_enriched, quality_stats = processor.compute_comment_quality(
    df_clean,
    comment_column='processed_comment'
)

print(f"""
ğŸ¤– AI Sentiment Analysis:
   - Mean quality: {quality_stats['mean_quality']:.3f}
   - Std quality: {quality_stats['std_quality']:.3f}
   - Range: [{quality_stats['min_quality']:.2f}, {quality_stats['max_quality']:.2f}]
""")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 2.3: USER SEGMENTATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
logger.info("Step 2.3: Segmenting users (trainable vs cold-start)...")
df_segmented, segment_stats = processor.segment_users(df_enriched)

print(f"""
ğŸ‘¥ User Segmentation:
   - Trainable users: {segment_stats['trainable_count']:,} ({segment_stats['trainable_pct']:.1f}%)
   - Cold-start users: {segment_stats['cold_start_count']:,} ({segment_stats['cold_start_pct']:.1f}%)
""")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 3: ID MAPPING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
logger.info("Step 3: Creating ID mappings...")
df_mapped, mappings = processor.create_id_mappings(df_segmented)

num_users = len(mappings['user_to_idx'])
num_items = len(mappings['item_to_idx'])
print(f"ğŸ”¢ Mappings: {num_users:,} users Ã— {num_items:,} items")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 4: TEMPORAL SPLIT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
logger.info("Step 4: Performing temporal split...")
df_split, split_stats = processor.temporal_split(
    df_mapped,
    method='leave_one_out',
    use_validation=False
)

print(f"""
ğŸ“… Temporal Split:
   - Train: {split_stats['train_size']:,} interactions
   - Test: {split_stats['test_size']:,} interactions
   - Sparsity: {split_stats.get('sparsity', 0):.4f}
""")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 5: MATRIX CONSTRUCTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
logger.info("Step 5: Building matrices...")

# Filter train data
df_train = df_split[df_split['split'] == 'train']

# Build CSR matrix for ALS (confidence scores)
X_confidence = processor.build_confidence_matrix(
    df_train, num_users, num_items,
    value_col='confidence_score'
)

# Build user positive sets
user_pos_sets = processor.build_user_positive_sets(df_train)

# Build item popularity
item_popularity = processor.build_item_popularity(df_train, num_items)

# Get top-K popular items
top_k_popular = processor.get_top_k_popular_items(df_train, k=50)

# Build hard negative sets
user_hard_neg_sets = processor.build_user_hard_negative_sets(
    df_train, top_k_popular
)

# Build user metadata
user_metadata = processor.build_user_metadata(df_split)

print(f"""
ğŸ“¦ Matrices Built:
   - X_confidence: {X_confidence.shape} (nnz: {X_confidence.nnz:,})
   - user_pos_sets: {len(user_pos_sets):,} users
   - user_hard_neg_sets: {len(user_hard_neg_sets):,} users
   - item_popularity: {len(item_popularity):,} items
""")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 6: SAVE ARTIFACTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
logger.info("Step 6: Saving artifacts...")

# Save all at once
saved_files = processor.save_all_artifacts(
    interactions_df=df_split,
    mappings=mappings,
    X_confidence=X_confidence,
    user_pos_sets=user_pos_sets,
    user_hard_neg_sets=user_hard_neg_sets,
    user_metadata=user_metadata,
    item_popularity=item_popularity,
    top_k_popular=top_k_popular,
    stats=split_stats
)

print(f"ğŸ’¾ Saved {len(saved_files)} artifacts")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 7: VERSIONING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
logger.info("Step 7: Creating data version...")

data_hash = processor.compute_data_hash()
version_id = processor.create_data_version(
    data_hash=data_hash,
    filters={
        'min_user_interactions': 2,
        'min_user_positives': 1,
        'positive_threshold': 4.0
    },
    files=list(saved_files.keys()),
    stats=split_stats
)

print(f"ğŸ“Œ Version created: {version_id}")
print("\nâœ… Pipeline completed successfully!")
```

### 4.3 Cháº¡y tá»« Script

```powershell
# Cháº¡y pipeline hoÃ n chá»‰nh
python scripts/run_task01_complete.py

# Hoáº·c vá»›i custom config
python scripts/run_task01_complete.py --config config/data_config.yaml
```

---

## 5. Chi Tiáº¿t CÃ¡c BÆ°á»›c Xá»­ LÃ½

### 5.1 Step 1: Data Validation

**Má»¥c Ä‘Ã­ch**: Äáº£m báº£o data quality trÆ°á»›c khi xá»­ lÃ½

**CÃ¡c kiá»ƒm tra thá»±c hiá»‡n**:

| Kiá»ƒm tra | HÃ nh Ä‘á»™ng | LÃ½ do |
|----------|-----------|-------|
| Missing `user_id`, `product_id` | Drop row | Cannot process without IDs |
| Missing `rating` | Drop row | Core feature required |
| Missing `cmt_date` (NaT) | Drop row | TrÃ¡nh data leakage |
| Rating ngoÃ i [1.0, 5.0] | Drop row | Invalid data |
| Duplicate (user, product) | Keep latest | Láº¥y interaction má»›i nháº¥t |

**Code Example**:
```python
from recsys.cf.data import DataAuditor

auditor = DataAuditor(rating_min=1.0, rating_max=5.0)
df_valid, report = auditor.validate(df_raw)
df_dedup, dedup_stats = auditor.deduplicate(df_valid, strategy='keep_latest')
outliers = auditor.detect_outliers(df_dedup)
```

### 5.2 Step 2: Feature Engineering (AI Sentiment)

**Má»¥c Ä‘Ã­ch**: Táº¡o `comment_quality` vÃ  `confidence_score` Ä‘á»ƒ phÃ¢n biá»‡t quality cá»§a reviews

**Model**: `5CD-AI/Vietnamese-Sentiment-visobert`
- Trained trÃªn 120K Vietnamese e-commerce reviews
- 3 classes: NEGATIVE, POSITIVE, NEUTRAL
- GPU batch processing (batch_size=64)

**Fake Review Detection Heuristics**:

| Heuristic | Bonus/Penalty | Äiá»u kiá»‡n |
|-----------|---------------|-----------|
| Long review | +0.1 | >25 words |
| Short review | -0.1 | <4 words |
| Positive keywords | +0.15 | "tháº¥m nhanh", "hiá»‡u quáº£"... |
| Negative keywords | -0.15 | "kÃ©m", "dá»Ÿ", "fake"... |
| Positive emojis | +0.1 | ğŸ˜â¤ï¸ğŸ‘âœ¨ğŸŒŸğŸ’¯ğŸ”¥ |
| Negative emojis | -0.1 | ğŸ˜¢ğŸ˜­ğŸ’”ğŸ‘ğŸ˜¡ |
| Rating-sentiment mismatch | -0.2 | High rating + negative text |
| Repetition (spam) | -0.15 | Low character diversity |

**Output Columns**:
- `comment_quality`: [0.0, 1.0] - Cháº¥t lÆ°á»£ng review
- `confidence_score`: [1.0, 6.0] - rating + comment_quality

**Code Example**:
```python
from recsys.cf.data import FeatureEngineer

engineer = FeatureEngineer(
    model_name="5CD-AI/Vietnamese-Sentiment-visobert",
    batch_size=64,
    no_comment_quality=0.5,
    enable_fake_review_checks=True
)

df_enriched, stats = engineer.compute_confidence_scores(
    df_clean, 
    comment_column='processed_comment'
)
```

### 5.3 Step 2.3: User Segmentation

**Má»¥c Ä‘Ã­ch**: PhÃ¢n loáº¡i users Ä‘á»ƒ quyáº¿t Ä‘á»‹nh serving strategy

**TiÃªu chÃ­ phÃ¢n loáº¡i**:

| Loáº¡i | Äiá»u kiá»‡n | % Users | Serving Strategy |
|------|-----------|---------|------------------|
| **Trainable** | â‰¥2 interactions AND â‰¥1 positive | ~8.6% | CF (ALS/BPR) |
| **Cold-start** | <2 interactions OR 0 positives | ~91.4% | Content-based + Popularity |

**Special Case**: User cÃ³ Ä‘Ãºng 2 interactions nhÆ°ng cáº£ 2 Ä‘á»u negative (rating <4) â†’ Force cold-start

**Code Example**:
```python
from recsys.cf.data import UserFilter

user_filter = UserFilter(
    min_interactions=2,
    min_positives=1,
    positive_threshold=4.0
)

df_segmented, stats = user_filter.segment_users(df_enriched)
# df_segmented cÃ³ cá»™t 'is_trainable_user' (True/False)
```

### 5.4 Step 3: ID Mapping

**Má»¥c Ä‘Ã­ch**: Chuyá»ƒn Ä‘á»•i original IDs sang contiguous indices cho sparse matrix

**Táº¡i sao cáº§n mapping?**:
- Original `user_id`: sparse (gaps, range 1-304708)
- Matrix cáº§n: contiguous (0 to num_users-1)
- Mapping cho phÃ©p O(1) lookup

**Output Structure**:
```json
{
  "user_to_idx": {"12345": 0, "67890": 1, ...},
  "idx_to_user": {"0": "12345", "1": "67890", ...},
  "item_to_idx": {"101": 0, "102": 1, ...},
  "idx_to_item": {"0": "101", "1": "102", ...}
}
```

### 5.5 Step 4: Temporal Split

**Má»¥c Ä‘Ã­ch**: Chia train/test theo thá»i gian, Ä‘áº£m báº£o no data leakage

**Method**: Leave-One-Out
- **Test**: Latest POSITIVE interaction per user
- **Train**: Táº¥t cáº£ interactions cÃ²n láº¡i
- **Validation** (optional): 2nd latest positive

**Key Features**:
- **Positive-only test**: Chá»‰ Ä‘o kháº£ nÄƒng recommend items user thÃ­ch
- **Vectorized**: 10-100x faster than iterative approach
- **Implicit negatives**: Sample 50 popular items user chÆ°a interact

**Code Example**:
```python
from recsys.cf.data import TemporalSplitter

splitter = TemporalSplitter(
    positive_threshold=4.0,
    implicit_negative_per_user=50,
    implicit_negative_strategy='popular'
)

df_split, stats = splitter.split(df_mapped, method='leave_one_out')
# df_split cÃ³ cá»™t 'split' = 'train' | 'test' | 'val'
```

### 5.6 Step 5: Matrix Construction

**Má»¥c Ä‘Ã­ch**: Build sparse matrices vÃ  auxiliary structures cho training

**Output Artifacts**:

| File | Type | Shape | Usage |
|------|------|-------|-------|
| `X_train_confidence.npz` | CSR matrix | (users, items) | ALS training |
| `user_pos_train.pkl` | Dict[int, Set] | - | Negative sampling |
| `user_hard_neg_train.pkl` | Dict[int, Dict] | - | Hard negative mining |
| `item_popularity.npy` | ndarray | (items,) | Popularity baseline |

**Code Example**:
```python
from recsys.cf.data import MatrixBuilder

builder = MatrixBuilder()

X_conf = builder.build_confidence_matrix(df_train, num_users, num_items)
user_pos = builder.build_user_positive_sets(df_train)
top_k = builder.get_top_k_popular_items(df_train, k=50)
user_neg = builder.build_user_hard_negative_sets(df_train, top_k)
```

### 5.7 Step 6 & 7: Save & Version

**Formats Used**:
- **Parquet**: Interactions (10x faster, 50% smaller)
- **JSON**: Mappings, stats (human-readable)
- **NPZ**: Sparse matrices (scipy format)
- **Pickle**: Python objects (sets, dicts)

**Versioning**:
- Má»—i version cÃ³ `data_hash` (MD5 cá»§a raw CSVs)
- Track `git_commit` cho reproducibility
- `is_stale()` check Ä‘á»ƒ trigger retraining

---

## 6. Output Artifacts

### 6.1 File Summary

| File | Size (approx) | Format | Description |
|------|---------------|--------|-------------|
| `interactions.parquet` | ~50 MB | Parquet | Full processed data |
| `user_item_mappings.json` | ~5 MB | JSON | ID mappings |
| `X_train_confidence.npz` | ~10 MB | NPZ | ALS matrix |
| `user_pos_train.pkl` | ~2 MB | Pickle | Positive sets |
| `user_hard_neg_train.pkl` | ~5 MB | Pickle | Hard negatives |
| `user_metadata.pkl` | ~1 MB | Pickle | User segmentation |
| `item_popularity.npy` | ~20 KB | NumPy | Popularity scores |
| `top_k_popular_items.json` | ~1 KB | JSON | Top-50 items |
| `data_stats.json` | ~5 KB | JSON | Statistics |
| `versions.json` | ~2 KB | JSON | Version history |

### 6.2 Loading Artifacts

```python
import pandas as pd
import numpy as np
import json
import pickle
from scipy.sparse import load_npz

# Load interactions
df = pd.read_parquet("data/processed/interactions.parquet")

# Load mappings
with open("data/processed/user_item_mappings.json") as f:
    mappings = json.load(f)

# Load confidence matrix
X_conf = load_npz("data/processed/X_train_confidence.npz")

# Load user sets
with open("data/processed/user_pos_train.pkl", "rb") as f:
    user_pos = pickle.load(f)

# Load popularity
popularity = np.load("data/processed/item_popularity.npy")
```

---

## 7. Cáº¥u HÃ¬nh NÃ¢ng Cao

### 7.1 Full Configuration Options

```python
processor = DataProcessor(
    # â•â•â• Data Paths â•â•â•
    base_path="data/published_data",
    output_path="data/processed",
    
    # â•â•â• Validation â•â•â•
    rating_min=1.0,
    rating_max=5.0,
    drop_missing_timestamps=True,  # CRITICAL: avoid data leakage
    
    # â•â•â• Thresholds â•â•â•
    positive_threshold=4.0,      # rating >= 4 â†’ positive
    hard_negative_threshold=3.0, # rating <= 3 â†’ hard negative
    
    # â•â•â• Comment Quality (FeatureEngineer) â•â•â•
    no_comment_quality=0.5,      # Default for missing comments
    sentiment_model="5CD-AI/Vietnamese-Sentiment-visobert",
    batch_size=64,               # GPU batch size
    enable_fake_review_checks=True,
    
    # â•â•â• User Filtering â•â•â•
    min_user_interactions=2,     # Min total interactions
    min_user_positives=1,        # Min positive interactions
    min_item_positives=5,        # Min positives per item
    
    # â•â•â• Temporal Split â•â•â•
    include_negative_holdout=True,
    implicit_negative_per_user=50,
    implicit_negative_strategy='popular',  # or 'random'
    
    # â•â•â• Versioning â•â•â•
    versions_file="versions.json",
    max_versions_kept=10
)
```

### 7.2 Disable AI Sentiment (CPU-only mode)

```python
# Náº¿u khÃ´ng cÃ³ GPU, cÃ³ thá»ƒ disable AI sentiment
processor = DataProcessor(
    # ... other config
    sentiment_model=None,  # Disable ViSoBERT
    enable_fake_review_checks=True  # Still use heuristics
)
```

### 7.3 Custom Keyword Dictionaries

```python
from recsys.cf.data import FeatureEngineer

engineer = FeatureEngineer(
    # Custom positive keywords
    positive_keywords={
        'tháº¥m nhanh', 'hiá»‡u quáº£', 'thÆ¡m', 'má»‹n', 'sÃ¡ng da',
        'tá»‘t', 'Æ°ng', 'recommend', 'mua láº¡i', 'hÃ ng auth'
    },
    # Custom negative keywords
    negative_keywords={
        'kÃ©m', 'dá»Ÿ', 'tháº¥t vá»ng', 'fake', 'giáº£', 'tá»‡',
        'khÃ´ng hiá»‡u quáº£', 'hÃ ng nhÃ¡i', 'lá»«a Ä‘áº£o'
    },
    # Custom emoji mappings
    positive_emojis={'ğŸ˜', 'â¤ï¸', 'ğŸ‘', 'âœ¨', 'ğŸŒŸ', 'ğŸ’¯'},
    negative_emojis={'ğŸ˜¢', 'ğŸ˜­', 'ğŸ’”', 'ğŸ‘', 'ğŸ˜¡'}
)
```

---

## 8. Xá»­ LÃ½ Lá»—i & Debug

### 8.1 Common Errors

| Error | NguyÃªn nhÃ¢n | Giáº£i phÃ¡p |
|-------|-------------|-----------|
| `UnicodeDecodeError` | CSV khÃ´ng pháº£i UTF-8 | Ensure `encoding='utf-8'` |
| `KeyError: 'processed_comment'` | Thiáº¿u column | Check CSV schema |
| `CUDA out of memory` | GPU VRAM khÃ´ng Ä‘á»§ | Giáº£m `batch_size` |
| `Empty DataFrame after filtering` | Threshold quÃ¡ strict | Giáº£m `min_user_interactions` |
| `Matrix shape mismatch` | ID mapping sai | Verify mappings |

### 8.2 Debug Mode

```python
import logging

# Enable debug logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/cf/data_processing.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# Processor sáº½ log chi tiáº¿t tá»«ng step
processor = DataProcessor(...)
```

### 8.3 Validation Checks

```python
# Sau khi cháº¡y pipeline, validate outputs
def validate_outputs(output_path="data/processed"):
    import pandas as pd
    from scipy.sparse import load_npz
    import json
    
    # 1. Check interactions
    df = pd.read_parquet(f"{output_path}/interactions.parquet")
    assert df['rating'].between(1, 5).all(), "Invalid ratings!"
    assert df['confidence_score'].between(1, 6).all(), "Invalid confidence!"
    assert not df['cmt_date'].isna().any(), "NaT timestamps found!"
    
    # 2. Check mappings alignment
    with open(f"{output_path}/user_item_mappings.json") as f:
        mappings = json.load(f)
    
    X = load_npz(f"{output_path}/X_train_confidence.npz")
    assert X.shape[0] <= len(mappings['user_to_idx']), "User mismatch!"
    assert X.shape[1] == len(mappings['item_to_idx']), "Item mismatch!"
    
    # 3. Check test set is positive-only
    df_test = df[df['split'] == 'test']
    assert (df_test['rating'] >= 4).all(), "Test contains negatives!"
    
    print("âœ… All validations passed!")

validate_outputs()
```

---

## 9. FAQ

### Q1: Táº¡i sao khÃ´ng dÃ¹ng rating trá»±c tiáº¿p lÃ m confidence?

**A**: VÃ¬ 95% ratings lÃ  5 sao, khÃ´ng cÃ³ discriminative power. Confidence score = rating + comment_quality cho phÃ©p phÃ¢n biá»‡t:
- 5 sao + review chi tiáº¿t â†’ confidence 5.8-6.0 (genuine)
- 5 sao + review ngáº¯n/spam â†’ confidence 5.0-5.3 (suspicious)

### Q2: Táº¡i sao threshold trainable lÃ  â‰¥2 interactions?

**A**: Trade-off giá»¯a data hunger vÃ  statistical viability:
- â‰¥3: Chá»‰ ~15K users (~5%), quÃ¡ Ã­t
- â‰¥2: ~26K users (~8.6%), Ä‘á»§ lá»›n vá»›i BERT initialization
- â‰¥1: Táº¥t cáº£ users nhÆ°ng khÃ´ng cÃ³ collaborative signal

### Q3: Cold-start users (91%) Ä‘Æ°á»£c serve nhÆ° tháº¿ nÃ o?

**A**: Content-based + Popularity:
1. PhoBERT item-item similarity (náº¿u cÃ³ lá»‹ch sá»­)
2. Popularity baseline (Top-50 popular items)
3. Hybrid reranking vá»›i weights: content=0.6, popularity=0.3, quality=0.1

### Q4: Implicit negatives dÃ¹ng lÃ m gÃ¬?

**A**: Äá»ƒ Ä‘Ã¡nh giÃ¡ model cÃ´ng báº±ng hÆ¡n:
- Test chá»‰ cÃ³ 1 positive per user
- Sample 50 popular items user chÆ°a mua lÃ m negatives
- TÃ­nh Recall@K, NDCG@K trÃªn set nÃ y

### Q5: LÃ m sao biáº¿t khi nÃ o cáº§n retrain?

**A**: DÃ¹ng VersionRegistry:
```python
if processor.is_data_version_stale(current_version, max_age_hours=168):  # 1 week
    print("Data is stale, trigger retraining!")
```

Hoáº·c monitor drift metrics trong `data_stats.json`.

---

## Changelog

| Version | Date | Changes |
|---------|------|---------|
| 1.0 | Jan 2025 | Initial release with all 7 steps |

---

## TÃ i Liá»‡u LiÃªn Quan

- [Task 01: Data Layer Specification](../tasks/01_data_layer.md)
- [Task 02: ALS/BPR Training](../tasks/02_cf_training.md)
- [Task 05: Serving Layer](../tasks/05_serving.md)
- [API Reference](./API_REFERENCE.md)

---

*Last updated: January 2025*
