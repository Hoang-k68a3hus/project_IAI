# H∆∞·ªõng D·∫´n Training: ALS & BPR Models

> **H∆∞·ªõng d·∫´n th·ª±c h√†nh training c√°c m√¥ h√¨nh Collaborative Filtering**  
> C·∫≠p nh·∫≠t: 2025-01-16

## üìã T·ªïng Quan

H∆∞·ªõng d·∫´n n√†y s·∫Ω ƒë∆∞a b·∫°n qua qu√° tr√¨nh training c√°c m√¥ h√¨nh ALS v√† BPR cho h·ªá th·ªëng g·ª£i √Ω m·ªπ ph·∫©m Vi·ªát Nam.

**Y√™u c·∫ßu:**
- Task 01 ƒë√£ ho√†n th√†nh (d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω trong `data/processed/`)
- Python 3.10+ v·ªõi dependencies ƒë√£ c√†i ƒë·∫∑t
- T·ªëi thi·ªÉu ~4GB RAM

---

## üöÄ B·∫Øt ƒê·∫ßu Nhanh

### Train M√¥ H√¨nh ALS (5 ph√∫t)

```python
from recsys.cf.model.als import (
    ALSMatrixPreparer,
    ALSModelInitializer,
    ALSTrainer,
    ALSEvaluator,
    save_als_complete
)

# B∆∞·ªõc 1: Load d·ªØ li·ªáu
preparer = ALSMatrixPreparer(base_path='data/processed')
data = preparer.prepare_complete_als_data()

# B∆∞·ªõc 2: Kh·ªüi t·∫°o m√¥ h√¨nh (d√πng preset sparse_data)
initializer = ALSModelInitializer(preset='sparse_data')
model = initializer.initialize_model()

# B∆∞·ªõc 3: Training
trainer = ALSTrainer(model=model)
result = trainer.fit(data.X_train_implicit)

# B∆∞·ªõc 4: ƒê√°nh gi√°
evaluator = ALSEvaluator(
    user_factors=model.user_factors,
    item_factors=model.item_factors,
    user_to_idx=data.mappings['user_to_idx'],
    idx_to_user=data.mappings['idx_to_user'],
    item_to_idx=data.mappings['item_to_idx'],
    idx_to_item=data.mappings['idx_to_item'],
    user_pos_train=data.user_pos_train,
    user_pos_test=data.user_pos_test
)
results = evaluator.evaluate(k_values=[10, 20], compare_baseline=True)
results.print_summary()

# B∆∞·ªõc 5: L∆∞u artifacts
artifacts = save_als_complete(
    user_embeddings=model.user_factors,
    item_embeddings=model.item_factors,
    params=initializer.config,
    metrics=results.metrics,
    validation_user_indices=list(data.user_pos_test.keys())[:1000],
    data_version_hash=data.metadata.get('data_hash', 'unknown'),
    output_dir='artifacts/cf/als'
)
```

### Train M√¥ H√¨nh BPR (20-30 ph√∫t)

```python
from recsys.cf.model.bpr import (
    BPRDataLoader,
    TripletSampler,
    HardNegativeMixer,
    BPRTrainer,
    save_bpr_complete
)
import numpy as np

# B∆∞·ªõc 1: Load d·ªØ li·ªáu
loader = BPRDataLoader(base_path='data/processed')
data = loader.load_all()

# B∆∞·ªõc 2: Kh·ªüi t·∫°o embeddings
rng = np.random.default_rng(42)
U = rng.normal(0, 0.01, (data.num_users, 64)).astype(np.float32)
V = rng.normal(0, 0.01, (data.num_items, 64)).astype(np.float32)

# B∆∞·ªõc 3: C·∫•u h√¨nh sampler
mixer = HardNegativeMixer(
    hard_neg_sets=data.hard_neg_sets,
    hard_ratio=0.3
)

# B∆∞·ªõc 4: Training
trainer = BPRTrainer(
    U=U,
    V=V,
    learning_rate=0.05,
    regularization=0.0001
)
history = trainer.fit(
    positive_pairs=data.positive_pairs,
    user_pos_sets=data.user_pos_sets,
    hard_neg_sets=data.hard_neg_sets,
    num_items=data.num_items,
    epochs=50,
    samples_per_epoch=5
)

# B∆∞·ªõc 5: L·∫•y embeddings cu·ªëi c√πng
U, V = trainer.get_embeddings()

# B∆∞·ªõc 6: L∆∞u artifacts
artifacts = save_bpr_complete(
    user_embeddings=U,
    item_embeddings=V,
    params={'factors': 64, 'lr': 0.05, 'reg': 0.0001, 'hard_ratio': 0.3},
    metrics={'best_epoch': history.get_best_epoch('recall@10')},
    training_history=history,
    data_version_hash='unknown',
    output_dir='artifacts/cf/bpr'
)
```

---

## üìä So S√°nh M√¥ H√¨nh

| Kh√≠a c·∫°nh | ALS | BPR |
|-----------|-----|-----|
| **Th·ªùi gian train** | 1-2 ph√∫t | 20-30 ph√∫t |
| **B·ªô nh·ªõ s·ª≠ d·ª•ng** | ~2GB | ~1GB |
| **Ph√π h·ª£p cho** | L·∫∑p nhanh | Ch·∫•t l∆∞·ª£ng ranking |
| **ƒêi·ªÉm m·∫°nh** | Nhanh, h·ªó tr·ª£ GPU | Hard negative mining |
| **ƒêi·ªÉm y·∫øu** | Point-wise loss | Training ch·∫≠m h∆°n |

---

## ‚öôÔ∏è C·∫•u H√¨nh Presets

### ALS Presets

| Preset | factors | regularization | alpha | Tr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng |
|--------|---------|----------------|-------|--------------------|
| `default` | 64 | 0.01 | 10 | S·ª≠ d·ª•ng chung |
| `sparse_data` | 64 | 0.10 | 5 | **Khuy·∫øn ngh·ªã** cho dataset n√†y |
| `high_quality` | 128 | 0.05 | 10 | Embeddings phong ph√∫ h∆°n |
| `fast` | 32 | 0.01 | 10 | Test nhanh |
| `normalized` | 64 | 0.01 | 40 | Cho confidence ƒë√£ chu·∫©n h√≥a |

**T·∫°i sao n√™n d√πng `sparse_data`?**
- D·ªØ li·ªáu c·ªßa ch√∫ng ta c√≥ m·∫≠t ƒë·ªô matrix ~0.11%
- Regularization cao h∆°n (Œª=0.1) ngƒÉn overfitting
- Alpha th·∫•p h∆°n (5) b√π ƒë·∫Øp cho confidence range 1-6

### C·∫•u H√¨nh BPR

**C∆° b·∫£n (SGD):**
```python
trainer = BPRTrainer(
    U=U, V=V,
    learning_rate=0.05,
    regularization=0.0001,
    lr_decay=0.9,
    lr_decay_every=10
)
```

**N√¢ng cao (AdamW + Dropout):**
```python
from recsys.cf.model.bpr import (
    AdvancedBPRTrainer,
    OptimizerConfig,
    TrainingConfig,
    OptimizerType
)

trainer = AdvancedBPRTrainer(
    U=U, V=V,
    optimizer_config=OptimizerConfig(
        optimizer_type=OptimizerType.ADAMW,
        learning_rate=0.01,
        weight_decay=0.01
    ),
    training_config=TrainingConfig(
        dropout_rate=0.1,
        gradient_clip=1.0
    )
)
```

---

## üéØ Hard Negative Sampling

### T·∫°i Sao Quan Tr·ªçng
- Random negatives qu√° d·ªÖ ‚Üí m√¥ h√¨nh kh√¥ng h·ªçc ƒë∆∞·ª£c s·ªü th√≠ch chi ti·∫øt
- Hard negatives bu·ªôc m√¥ h√¨nh ph√¢n bi·ªát c√°c items t∆∞∆°ng t·ª±

### Chi·∫øn L∆∞·ª£c (30% hard + 70% random)

```python
mixer = HardNegativeMixer(
    hard_neg_sets=data.hard_neg_sets,
    hard_ratio=0.3  # 30% from hard negatives
)
```

**Ngu·ªìn hard negatives:**
1. **Explicit**: Items user ƒë√°nh gi√° ‚â§3 sao (kh√¥ng th√≠ch r√µ r√†ng)
2. **Implicit**: Top-50 items ph·ªï bi·∫øn user kh√¥ng mua (t·ª´ ch·ªëi ng·∫ßm)

### Theo D√µi Th·ªëng K√™ Sampling

```python
# Sau khi training
stats = mixer.get_stats()
print(f"Hard samples: {stats['hard_count']} ({stats['hard_ratio']:.1%})")
print(f"Random samples: {stats['random_count']} ({stats['random_ratio']:.1%})")
print(f"Fallbacks: {stats['fallback_count']}")
```

---

## üìà Theo D√µi Training

### Ti·∫øn Tr√¨nh ALS

```python
trainer = ALSTrainer(
    model=model,
    track_memory=True,
    checkpoint_interval=5
)
result = trainer.fit(X_train)

print(f"Training time: {result.training_time:.1f}s")
print(f"Peak memory: {result.memory_usage['peak_mb']:.1f} MB")
```

### Ti·∫øn Tr√¨nh BPR

```python
# Training history theo d√µi:
# - losses: BPR loss m·ªói epoch
# - val_metrics: Validation Recall@K, NDCG@K
# - learning_rates: L·ªãch LR
# - durations: Th·ªùi gian m·ªói epoch

history = trainer.fit(...)

# V·∫Ω learning curve
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# ƒê·ªì th·ªã Loss
axes[0].plot(history.epochs, history.losses)
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('BPR Loss')
axes[0].set_title('Training Loss')

# ƒê·ªì th·ªã Recall
if 'recall@10' in history.val_metrics:
    axes[1].plot(history.epochs, history.val_metrics['recall@10'])
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Recall@10')
    axes[1].set_title('Validation Recall')

plt.tight_layout()
plt.savefig('training_curves.png')
```

---

## üîß X·ª≠ L√Ω S·ª± C·ªë

### L·ªói B·ªô Nh·ªõ

**ALS h·∫øt b·ªô nh·ªõ (OOM):**
```python
# C√°ch 1: Gi·∫£m factors
initializer = ALSModelInitializer(preset='fast')  # factors=32

# C√°ch 2: B·∫≠t GPU (c·∫ßn cupy)
initializer = ALSModelInitializer(config={
    'factors': 64,
    'use_gpu': True
})
```

**BPR h·∫øt b·ªô nh·ªõ (OOM):**
```python
# Gi·∫£m samples m·ªói epoch
trainer.fit(
    ...,
    samples_per_epoch=3  # Thay v√¨ 5
)
```

### Training Kh√¥ng ·ªîn ƒê·ªãnh

**Loss dao ƒë·ªông (BPR):**
```python
# Gi·∫£m learning rate
trainer = BPRTrainer(
    U=U, V=V,
    learning_rate=0.01,  # Gi·∫£m t·ª´ 0.05
    lr_decay=0.95,       # Decay m·∫°nh h∆°n
    lr_decay_every=5
)
```

**NaN trong embeddings:**
```python
# TƒÉng regularization
trainer = BPRTrainer(
    U=U, V=V,
    regularization=0.001  # TƒÉng t·ª´ 0.0001
)

# Ho·∫∑c d√πng gradient clipping
from recsys.cf.model.bpr import AdvancedBPRTrainer, TrainingConfig

trainer = AdvancedBPRTrainer(
    U=U, V=V,
    training_config=TrainingConfig(gradient_clip=1.0)
)
```

### Metrics Th·∫•p

**Recall@10 d∆∞·ªõi baseline:**
1. Ki·ªÉm tra test set ch·ªâ c√≥ positive (rating ‚â•4)
2. X√°c nh·∫≠n seen item filtering ho·∫°t ƒë·ªông
3. Th·ª≠ factors cao h∆°n (128) ho·∫∑c regularization th·∫•p h∆°n

```python
# Debug: Ki·ªÉm tra c·∫•u h√¨nh evaluation
print(f"Test users: {len(user_pos_test)}")
print(f"Trung b√¨nh test items m·ªói user: {np.mean([len(v) for v in user_pos_test.values()]):.1f}")
print(f"Train items ƒë√£ l·ªçc: {sum(len(v) for v in user_pos_train.values())}")
```

---

## üß™ Tuning Hyperparameter

### V√≠ D·ª• Grid Search

```python
from itertools import product
import pandas as pd

# ALS grid
param_grid = {
    'factors': [32, 64, 128],
    'regularization': [0.01, 0.05, 0.1],
    'alpha': [5, 10, 20]
}

results = []
for factors, reg, alpha in product(*param_grid.values()):
    # Training
    initializer = ALSModelInitializer(config={
        'factors': factors,
        'regularization': reg,
        'alpha': alpha,
        'iterations': 15
    })
    model = initializer.initialize_model()
    trainer = ALSTrainer(model=model)
    trainer.fit(data.X_train_implicit)
    
    # ƒê√°nh gi√°
    evaluator = ALSEvaluator(...)
    metrics = evaluator.evaluate(k_values=[10])
    
    results.append({
        'factors': factors,
        'regularization': reg,
        'alpha': alpha,
        'recall@10': metrics.metrics['recall@10']
    })

# T√¨m config t·ªët nh·∫•t
df = pd.DataFrame(results)
best = df.loc[df['recall@10'].idxmax()]
print(f"Config t·ªët nh·∫•t: {best.to_dict()}")
```

---

## üì¶ L∆∞u & Load

### L∆∞u To√†n B·ªô Artifacts

```python
# ALS
from recsys.cf.model.als import save_als_complete

artifacts = save_als_complete(
    user_embeddings=model.user_factors,
    item_embeddings=model.item_factors,
    params={'factors': 64, 'regularization': 0.1, 'alpha': 10},
    metrics={'recall@10': 0.234, 'ndcg@10': 0.189},
    validation_user_indices=list(user_pos_test.keys())[:1000],
    data_version_hash='abc123',
    output_dir='artifacts/cf/als'
)

# C√°c file ƒë∆∞·ª£c t·∫°o:
# - artifacts/cf/als/als_U.npy
# - artifacts/cf/als/als_V.npy
# - artifacts/cf/als/als_params.json
# - artifacts/cf/als/als_metrics.json
# - artifacts/cf/als/als_metadata.json (bao g·ªìm score_range)
```

### Load Cho Serving

```python
import numpy as np
import json

# Load embeddings
U = np.load('artifacts/cf/als/als_U.npy')
V = np.load('artifacts/cf/als/als_V.npy')

# Load metadata
with open('artifacts/cf/als/als_metadata.json') as f:
    metadata = json.load(f)

# L·∫•y score range cho normalization (Task 08)
score_range = metadata['score_range']
print(f"Score range: [{score_range['p01']:.3f}, {score_range['p99']:.3f}]")
```

---

## üîó T√≠ch H·ª£p V·ªõi Task 08

### Score Range Cho Hybrid Reranking

C·∫£ ALS v√† BPR ƒë·ªÅu l∆∞u `score_range` trong metadata cho Task 08 hybrid reranking:

```python
# Trong qu√° tr√¨nh training/saving
artifacts = save_als_complete(
    ...,
    validation_user_indices=[10, 25, 42, ...],  # Quan tr·ªçng!
    ...
)

# Score range ƒë∆∞·ª£c t√≠nh b·∫±ng U @ V.T tr√™n validation users
# Cung c·∫•p p01, p99 percentiles cho normalization ·ªïn ƒë·ªãnh
```

### S·ª≠ D·ª•ng Trong Task 08

```python
def normalize_cf_scores(scores, score_range):
    """Chu·∫©n h√≥a CF scores v·ªÅ [0, 1] d√πng p01-p99 range."""
    p01, p99 = score_range['p01'], score_range['p99']
    normalized = (scores - p01) / (p99 - p01)
    return np.clip(normalized, 0, 1)
```

---

## üìö T√†i Li·ªáu Li√™n Quan

- [Task 02: Training Pipelines](../tasks/02_training_pipelines.md) - ƒê·∫∑c t·∫£ k·ªπ thu·∫≠t ƒë·∫ßy ƒë·ªß
- [H∆∞·ªõng D·∫´n X·ª≠ L√Ω D·ªØ Li·ªáu](DATA_PROCESSING_GUIDE.md) - Outputs c·ªßa Task 01
- [API Reference](API_REFERENCE.md) - C√°c endpoints ph·ª•c v·ª•

---

---

## üìè ƒê√°nh Gi√° M√¥ H√¨nh (Evaluation)

### T·ªïng Quan Metrics

| Metric | C√¥ng Th·ª©c | M√¥ T·∫£ |
|--------|-----------|-------|
| **Recall@K** | `\|Top-K ‚à© Test\| / \|Test\|` | T·ª∑ l·ªá items test t√¨m th·∫•y trong top-K |
| **NDCG@K** | `DCG@K / IDCG@K` | Ch·∫•t l∆∞·ª£ng ranking (items ·ªü top ƒë∆∞·ª£c reward) |
| **Precision@K** | `\|Top-K ‚à© Test\| / K` | ƒê·ªô ch√≠nh x√°c c·ªßa top-K |
| **Coverage** | `\|Unique Recs\| / \|All Items\|` | ƒêa d·∫°ng: % items ƒë∆∞·ª£c recommend |

### ƒê√°nh Gi√° Nhanh V·ªõi ALSEvaluator

```python
from recsys.cf.model.als import ALSEvaluator

# Kh·ªüi t·∫°o evaluator
evaluator = ALSEvaluator(
    user_factors=model.user_factors,
    item_factors=model.item_factors,
    user_to_idx=data.mappings['user_to_idx'],
    idx_to_user=data.mappings['idx_to_user'],
    item_to_idx=data.mappings['item_to_idx'],
    idx_to_item=data.mappings['idx_to_item'],
    user_pos_train=data.user_pos_train,
    user_pos_test=data.user_pos_test
)

# Ch·∫°y evaluation v·ªõi so s√°nh baseline
results = evaluator.evaluate(
    k_values=[10, 20],          # K values c·∫ßn ƒë√°nh gi√°
    filter_seen=True,            # L·ªçc items ƒë√£ th·∫•y trong train
    compare_baseline=True,       # So s√°nh v·ªõi popularity baseline
    baseline_source='train',     # Ngu·ªìn popularity: 'train' ho·∫∑c 'metadata'
    model_type='als'
)

# In k·∫øt qu·∫£ d·∫°ng b·∫£ng
results.print_summary()
```

**Output m·∫´u:**
```
======================================================================
EVALUATION RESULTS: ALS
======================================================================

Test Users: 26234
K Values: [10, 20]
Evaluation Time: 45.23s

----------------------------------------------------------------------
Metric               Model        Baseline     Improvement    
----------------------------------------------------------------------
recall@10            0.2453       0.1421       +72.6%         
ndcg@10              0.1892       0.1024       +84.8%         
recall@20            0.3124       0.2013       +55.2%         
ndcg@20              0.2215       0.1342       +65.1%         
======================================================================
```

### ƒê√°nh Gi√° ƒê∆°n Gi·∫£n (Kh√¥ng C·∫ßn Full Mappings)

```python
from recsys.cf.model.als import quick_evaluate

metrics = quick_evaluate(
    user_factors=U,
    item_factors=V,
    user_pos_test=user_pos_test,
    user_pos_train=user_pos_train,
    k_values=[10, 20],
    model_type='als'
)

print(f"Recall@10: {metrics['recall@10']:.4f}")
print(f"NDCG@10: {metrics['ndcg@10']:.4f}")
```

### So S√°nh V·ªõi Popularity Baseline

```python
from recsys.cf.model.als import PopularityBaseline

# Kh·ªüi t·∫°o baseline t·ª´ training data
baseline = PopularityBaseline()
baseline.fit_from_train(
    user_pos_train=user_pos_train,
    num_items=2231
)

# Ho·∫∑c t·ª´ product metadata (num_sold_time)
# baseline.fit_from_metadata(
#     product_df=product_df,
#     item_to_idx=item_to_idx,
#     popularity_col='num_sold_time'
# )

# ƒê√°nh gi√° baseline
baseline_metrics = baseline.evaluate(
    user_pos_test=user_pos_test,
    user_pos_train=user_pos_train,
    k_values=[10, 20],
    filter_seen=True
)

print(f"Baseline Recall@10: {baseline_metrics['recall@10']:.4f}")
```

**Expected Performance:**
| M√¥ H√¨nh | Recall@10 | NDCG@10 | Coverage |
|---------|-----------|---------|----------|
| **Popularity Baseline** | 0.12-0.15 | 0.08-0.10 | <0.05 |
| **ALS** | >0.20 | >0.16 | 0.25-0.35 |
| **BPR** | >0.22 | >0.18 | 0.28-0.38 |

### ƒê√°nh Gi√° Hybrid (CF + BERT Reranking)

```bash
# So s√°nh pure CF vs hybrid reranking
python scripts/evaluate_hybrid.py --num-users 500 --topk 10 20

# Ch·ªâ ƒë√°nh gi√° pure CF
python scripts/evaluate_hybrid.py --cf-only --num-users 500

# L∆∞u k·∫øt qu·∫£
python scripts/evaluate_hybrid.py --output reports/hybrid_eval.json
```

**C√°c metrics hybrid b·ªï sung:**
| Metric | M√¥ T·∫£ |
|--------|-------|
| **Diversity** | 1 - avg pairwise similarity (BERT) trong top-K |
| **Semantic Alignment** | Cosine similarity gi·ªØa user profile v√† recommendations |
| **Brand Coverage** | % brands ƒëa d·∫°ng trong recommendations |

```python
# S·ª≠ d·ª•ng HybridEvaluator tr·ª±c ti·∫øp
from scripts.evaluate_hybrid import HybridEvaluator, compare_cf_vs_hybrid

# So s√°nh CF vs Hybrid
comparison = compare_cf_vs_hybrid(
    cf_recommender=recommender,
    test_data=test_data,
    phobert_loader=phobert_loader,
    metadata=product_df,
    num_users=200,
    k_values=[5, 10, 20]
)

print(f"Diversity improvement: {comparison['summary']['diversity_improvement']:+.1f}%")
print(f"Recall@10 improvement: {comparison['summary']['recall@10_improvement']:+.1f}%")
```

### L∆∞u K·∫øt Qu·∫£ Evaluation

```python
from pathlib import Path

# L∆∞u k·∫øt qu·∫£ evaluation
results.save(Path('artifacts/cf/als/als_eval_results.json'))

# Ho·∫∑c th·ªß c√¥ng
import json

eval_output = {
    'model_type': 'als',
    'metrics': {
        'recall@10': 0.2453,
        'ndcg@10': 0.1892,
        'coverage': 0.312
    },
    'baseline': {
        'recall@10': 0.1421,
        'ndcg@10': 0.1024
    },
    'improvement': {
        'recall@10': '+72.6%',
        'ndcg@10': '+84.8%'
    },
    'num_test_users': 26234,
    'evaluation_time_seconds': 45.23
}

with open('reports/als_eval.json', 'w', encoding='utf-8') as f:
    json.dump(eval_output, f, indent=2, ensure_ascii=False)
```

### Ph√¢n T√≠ch Per-User (Advanced)

```python
import numpy as np
from collections import defaultdict

# Thu th·∫≠p metrics theo t·ª´ng user
per_user_metrics = defaultdict(list)

for user_idx, ground_truth in user_pos_test.items():
    scores = model.user_factors[user_idx] @ model.item_factors.T
    
    # L·ªçc seen items
    if user_idx in user_pos_train:
        scores[list(user_pos_train[user_idx])] = -np.inf
    
    # Top-K predictions
    top_k = np.argpartition(scores, -10)[-10:]
    predictions = top_k[np.argsort(scores[top_k])[::-1]]
    
    # Recall@10
    hits = len(set(predictions) & ground_truth)
    recall = hits / len(ground_truth)
    per_user_metrics['recall@10'].append(recall)
    per_user_metrics['user_idx'].append(user_idx)

# Ph√¢n t√≠ch distribution
recalls = np.array(per_user_metrics['recall@10'])
print(f"Recall@10 Mean: {recalls.mean():.4f}")
print(f"Recall@10 Std: {recalls.std():.4f}")
print(f"Recall@10 Median: {np.median(recalls):.4f}")
print(f"Users v·ªõi Recall=0: {(recalls == 0).sum()} ({(recalls == 0).mean()*100:.1f}%)")
```

### Stratification Theo User Activity

```python
# Ph√¢n nh√≥m users theo s·ªë interactions
def stratified_evaluation(user_pos_train, user_pos_test, metrics):
    """ƒê√°nh gi√° theo activity level."""
    
    # Ph√¢n nh√≥m
    low_activity = []     # 2-5 interactions
    medium_activity = []  # 6-15 interactions
    high_activity = []    # >15 interactions
    
    user_recalls = dict(zip(
        per_user_metrics['user_idx'], 
        per_user_metrics['recall@10']
    ))
    
    for user_idx, train_items in user_pos_train.items():
        if user_idx not in user_recalls:
            continue
        
        recall = user_recalls[user_idx]
        num_train = len(train_items)
        
        if num_train <= 5:
            low_activity.append(recall)
        elif num_train <= 15:
            medium_activity.append(recall)
        else:
            high_activity.append(recall)
    
    print(f"Low activity (2-5): Recall@10 = {np.mean(low_activity):.4f} ({len(low_activity)} users)")
    print(f"Medium activity (6-15): Recall@10 = {np.mean(medium_activity):.4f} ({len(medium_activity)} users)")
    print(f"High activity (>15): Recall@10 = {np.mean(high_activity):.4f} ({len(high_activity)} users)")

stratified_evaluation(user_pos_train, user_pos_test, per_user_metrics)
```

**Expected Output:**
```
Low activity (2-5): Recall@10 = 0.1823 (18234 users)
Medium activity (6-15): Recall@10 = 0.2891 (6521 users)
High activity (>15): Recall@10 = 0.3542 (1479 users)
```

### Ki·ªÉm Tra Statistical Significance

```python
from scipy import stats

# Paired t-test: CF vs Baseline
cf_recalls = per_user_cf['recall@10']
baseline_recalls = per_user_baseline['recall@10']

t_stat, p_value = stats.ttest_rel(cf_recalls, baseline_recalls)

print(f"Paired t-test:")
print(f"  t-statistic: {t_stat:.4f}")
print(f"  p-value: {p_value:.2e}")

if p_value < 0.05:
    print("  ‚Üí Significant improvement (p < 0.05)")
else:
    print("  ‚Üí Not significant")

# Effect size (Cohen's d)
mean_diff = np.mean(cf_recalls) - np.mean(baseline_recalls)
pooled_std = np.sqrt((np.std(cf_recalls)**2 + np.std(baseline_recalls)**2) / 2)
cohens_d = mean_diff / pooled_std

print(f"\nEffect size (Cohen's d): {cohens_d:.3f}")
if abs(cohens_d) < 0.2:
    print("  ‚Üí Small effect")
elif abs(cohens_d) < 0.5:
    print("  ‚Üí Medium effect")
else:
    print("  ‚Üí Large effect")
```

### Debug: Inspect Recommendations

```python
import pandas as pd

def inspect_user_recommendations(
    user_id: str,
    model,
    mappings,
    products_df,
    user_pos_train,
    k: int = 10
):
    """Debug recommendations cho 1 user c·ª• th·ªÉ."""
    
    # Map user_id ‚Üí u_idx
    if user_id not in mappings['user_to_idx']:
        print(f"User {user_id} kh√¥ng t√¨m th·∫•y")
        return
    
    u_idx = mappings['user_to_idx'][user_id]
    
    # Compute scores
    scores = model.user_factors[u_idx] @ model.item_factors.T
    
    # Get seen items
    seen_items = user_pos_train.get(u_idx, set())
    print(f"User {user_id} (u_idx={u_idx}) ƒë√£ th·∫•y {len(seen_items)} items")
    
    # Filter seen
    scores[list(seen_items)] = -np.inf
    
    # Top-K
    top_k = np.argsort(scores)[::-1][:k]
    
    # Map to product_ids
    product_ids = [mappings['idx_to_item'][i] for i in top_k]
    
    # Get product info
    recs = products_df[products_df['product_id'].isin(product_ids)]
    recs = recs[['product_id', 'product_name', 'brand', 'avg_star', 'num_sold_time']]
    
    print(f"\nTop-{k} Recommendations:")
    for rank, (idx, row) in enumerate(recs.iterrows(), 1):
        print(f"  {rank}. {row['product_name'][:50]}...")
        print(f"     Brand: {row['brand']}, Rating: {row['avg_star']:.1f}, Sold: {row['num_sold_time']}")
    
    return recs

# Usage
# inspect_user_recommendations('12345', model, mappings, products_df, user_pos_train)
```

---

## üóÑÔ∏è Model Registry & Versioning

### T·ªïng Quan Registry

Model Registry qu·∫£n l√Ω t·∫•t c·∫£ c√°c phi√™n b·∫£n models ƒë√£ train, theo d√µi performance, v√† t·ª± ƒë·ªông ch·ªçn "best model" cho production serving.

**C·∫•u tr√∫c th∆∞ m·ª•c:**
```
artifacts/cf/
‚îú‚îÄ‚îÄ als/
‚îÇ   ‚îú‚îÄ‚îÄ v1_20250115_103000/      # Version 1
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ als_U.npy
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ als_V.npy
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ als_params.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ als_metadata.json
‚îÇ   ‚îî‚îÄ‚îÄ v2_20250116_141500/      # Version 2
‚îú‚îÄ‚îÄ bpr/
‚îÇ   ‚îî‚îÄ‚îÄ v1_20250115_120000/
‚îú‚îÄ‚îÄ registry.json                 # CF models registry
‚îî‚îÄ‚îÄ bert_registry.json           # BERT embeddings registry
```

### ƒêƒÉng K√Ω Model M·ªõi

```python
from recsys.cf.registry import ModelRegistry

registry = ModelRegistry(registry_path='artifacts/cf/registry.json')

# ƒêƒÉng k√Ω model sau khi train
model_id = registry.register_model(
    artifacts_path='artifacts/cf/als/v2_20250116_141500',
    model_type='als',
    hyperparameters={
        'factors': 128,
        'regularization': 0.01,
        'iterations': 20,
        'alpha': 60
    },
    metrics={
        'recall@10': 0.245,
        'ndcg@10': 0.195,
        'coverage': 0.310
    },
    training_info={
        'training_time_seconds': 102.8,
        'num_users': 26000,
        'num_items': 2200
    },
    data_version='abc123...',     # Hash t·ª´ Task 01
    git_commit='ghi789...',       # Auto-detect n·∫øu None
    baseline_comparison={
        'baseline_type': 'popularity',
        'improvement_ndcg@10': 0.912  # 91.2% improvement
    }
)

print(f"Registered model: {model_id}")
# Output: Registered model: als_v2_20250116_141500
```

### Ch·ªçn Best Model T·ª± ƒê·ªông

```python
# Ch·ªçn model t·ªët nh·∫•t theo metric
best = registry.select_best_model(
    metric='ndcg@10',              # Metric ƒë·ªÉ so s√°nh
    min_improvement=0.1,           # T·ªëi thi·ªÉu 10% improvement so v·ªõi baseline
    model_type=None                # None = t·∫•t c·∫£ types (als, bpr)
)

print(f"Best model: {best['model_id']}")
print(f"NDCG@10: {best['value']:.4f}")

# Output:
# Best model: als_v2_20250116_141500
# NDCG@10: 0.1950
```

### Li·ªát K√™ & So S√°nh Models

```python
import pandas as pd

# Li·ªát k√™ t·∫•t c·∫£ models
df = registry.list_models()

# L·ªçc theo lo·∫°i v√† status
als_models = registry.list_models(
    model_type='als',
    status='active',
    sort_by='ndcg@10',
    ascending=False
)

print(als_models[['model_id', 'ndcg@10', 'recall@10', 'training_time']])
```

**Output m·∫´u:**
```
                model_id  ndcg@10  recall@10  training_time
0  als_v2_20250116_141500   0.195      0.245          102.8
1  als_v1_20250115_103000   0.189      0.234           45.2
```

```python
# So s√°nh chi ti·∫øt c√°c models
comparison = registry.compare_models(
    model_ids=['als_v1_20250115_103000', 'als_v2_20250116_141500', 'bpr_v1_20250115_120000'],
    metrics=['recall@10', 'ndcg@10', 'coverage']
)

print(comparison)
```

### Load Model Cho Serving

```python
from recsys.cf.registry import ModelLoader, get_loader

# C√°ch 1: Singleton pattern (khuy·∫øn ngh·ªã cho serving)
loader = get_loader()

# Load current best model
U, V, metadata = loader.load_current_best()
print(f"Loaded model: {metadata['model_id']}")
print(f"Embeddings shape: U={U.shape}, V={V.shape}")

# C√°ch 2: Load model c·ª• th·ªÉ
U, V, metadata = loader.load_model('als_v2_20250116_141500')

# Quick access embeddings
U, V = loader.get_embeddings()
```

### Hot-Reload Model (Kh√¥ng Downtime)

```python
# Ki·ªÉm tra v√† reload n·∫øu c√≥ model m·ªõi
model_changed = loader.reload_model()

if model_changed:
    print("Model updated!")
    U, V, metadata = loader.load_current_best()
else:
    print("Model unchanged")

# Xem th·ªëng k√™ loader
stats = loader.get_stats()
print(f"Total loads: {stats['total_loads']}")
print(f"Cache hit rate: {stats['cache_hit_rate']:.2%}")
print(f"Last reload: {stats['last_reload_at']}")
```

### Archive & Delete Models

```python
# Archive model c≈© (kh√¥ng x√≥a files)
success = registry.archive_model('als_v1_20250115_103000')
# Model archived ‚Üí kh√¥ng xu·∫•t hi·ªán trong best model selection

# X√≥a model (c·∫©n th·∫≠n!)
success = registry.delete_model(
    model_id='als_v1_20250115_103000',
    delete_files=False  # True ƒë·ªÉ x√≥a c·∫£ files tr√™n disk
)
```

**L∆∞u √Ω:**
- Kh√¥ng th·ªÉ archive/delete model ƒëang l√† `current_best`
- T·∫•t c·∫£ thao t√°c ƒë∆∞·ª£c ghi v√†o `logs/registry_audit.log`

### Registry Statistics

```python
stats = registry.get_registry_stats()

print(f"Total models: {stats['total_models']}")
print(f"Active models: {stats['active_models']}")
print(f"Archived models: {stats['archived_models']}")
print(f"By type: {stats['by_type']}")  # {'als': 2, 'bpr': 1}
print(f"Current best: {stats['current_best']}")
```

### BERT Embeddings Registry (Ri√™ng Bi·ªát)

```python
from recsys.cf.registry import BERTEmbeddingsRegistry, get_bert_registry

bert_registry = get_bert_registry()

# ƒêƒÉng k√Ω BERT embeddings
version = bert_registry.register_embeddings(
    embedding_path='data/processed/content_based_embeddings',
    model_name='vinai/phobert-base',
    num_items=2244,
    embedding_dim=768,
    generation_config={
        'batch_size': 32,
        'max_length': 256
    },
    text_fields_used=['product_name', 'description', 'ingredients']
)

# Load embeddings
from recsys.cf.registry import load_bert_embeddings

embeddings, metadata = load_bert_embeddings()  # Current best
# ho·∫∑c
embeddings, metadata = load_bert_embeddings(version='bert_20250115_103000')
```

### Utility Functions

```python
from recsys.cf.registry.utils import (
    generate_version_id,
    compute_data_version,
    get_git_commit,
    backup_registry,
    restore_registry
)

# T·∫°o version ID
version = generate_version_id(prefix='v')  # ‚Üí 'v_20250116_141500'

# Compute data version (hash)
data_version = compute_data_version([
    'data/processed/interactions.parquet',
    'data/processed/user_item_mappings.json'
])

# Get git commit
commit = get_git_commit()  # ‚Üí 'def4567890abcdef...'

# Backup registry
backup_path = backup_registry('artifacts/cf/registry.json')

# Restore t·ª´ backup
restore_registry(
    backup_path='artifacts/cf/registry_backup_20250116_120000.json',
    registry_path='artifacts/cf/registry.json',
    create_current_backup=True
)
```

### T√≠ch H·ª£p V·ªõi Training Pipeline

```python
from recsys.cf.registry import ModelRegistry
from recsys.cf.registry.utils import compute_data_version, get_git_commit

# Sau khi train xong
def register_trained_model(output_path, model_type, params, metrics, elapsed_time):
    """ƒêƒÉng k√Ω model v√†o registry sau khi train."""
    
    registry = ModelRegistry()
    
    # Compute data version
    data_version = compute_data_version([
        'data/processed/interactions.parquet',
        'data/processed/user_item_mappings.json'
    ])
    
    # Register
    model_id = registry.register_model(
        artifacts_path=output_path,
        model_type=model_type,
        hyperparameters=params,
        metrics=metrics,
        training_info={
            'training_time_seconds': elapsed_time,
            'num_users': metrics.get('num_users', 0),
            'num_items': metrics.get('num_items', 0)
        },
        data_version=data_version,
        git_commit=get_git_commit()
    )
    
    return model_id

# Usage trong training script
model_id = register_trained_model(
    output_path='artifacts/cf/als/v2_20250116_141500',
    model_type='als',
    params={'factors': 128, 'regularization': 0.01},
    metrics={'recall@10': 0.245, 'ndcg@10': 0.195},
    elapsed_time=102.8
)

# Auto-select n·∫øu metrics t·ªët h∆°n
best = registry.select_best_model(metric='ndcg@10', min_improvement=0.05)
if best and best['model_id'] == model_id:
    print(f"üéâ New best model selected: {model_id}")
```

### Registry Schema (registry.json)

```json
{
  "current_best": {
    "model_id": "als_v2_20250116_141500",
    "model_type": "als",
    "version": "v2_20250116_141500",
    "path": "artifacts/cf/als/v2_20250116_141500",
    "selection_metric": "ndcg@10",
    "selection_value": 0.195,
    "selected_at": "2025-01-16T14:30:00"
  },
  
  "models": {
    "als_v2_20250116_141500": {
      "model_type": "als",
      "version": "v2_20250116_141500",
      "path": "artifacts/cf/als/v2_20250116_141500",
      "created_at": "2025-01-16T14:15:00",
      "data_version": "abc123...",
      "git_commit": "ghi789...",
      "hyperparameters": {
        "factors": 128,
        "regularization": 0.01,
        "iterations": 20,
        "alpha": 60
      },
      "metrics": {
        "recall@10": 0.245,
        "ndcg@10": 0.195,
        "coverage": 0.310
      },
      "baseline_comparison": {
        "baseline_type": "popularity",
        "improvement_ndcg@10": 0.912
      },
      "status": "active"
    }
  },
  
  "metadata": {
    "registry_version": "1.0",
    "last_updated": "2025-01-16T14:30:00",
    "num_models": 3,
    "selection_criteria": "ndcg@10"
  }
}
```

### Scripts Ti·ªán √çch

```bash
# Update registry v·ªõi model m·ªõi v√† auto-select
python scripts/update_registry.py \
  --model-path artifacts/cf/als/v2_20250116_141500 \
  --auto-select

# Cleanup models c≈© (gi·ªØ 5 g·∫ßn nh·∫•t)
python scripts/cleanup_old_models.py \
  --keep-last 5 \
  --archive-old  # Archive thay v√¨ delete
```

### Audit Trail

T·∫•t c·∫£ thao t√°c ƒë∆∞·ª£c ghi v√†o `logs/registry_audit.log`:

```
2025-01-15 10:30:00 | REGISTER | als_v1_20250115_103000 | ndcg@10=0.189
2025-01-16 14:15:00 | REGISTER | als_v2_20250116_141500 | ndcg@10=0.195
2025-01-16 14:30:00 | SELECT_BEST | als_v2_20250116_141500 | improvement=+3.2%
2025-01-17 09:00:00 | ARCHIVE | als_v1_20250115_103000 | reason=superseded
```

---

## ‚úÖ Checklist

Tr∆∞·ªõc khi deploy m√¥ h√¨nh ƒë√£ train:

- [ ] Recall@10 v∆∞·ª£t popularity baseline ‚â•20%
- [ ] Score range ƒë√£ t√≠nh v√† l∆∞u trong metadata
- [ ] Training ho√†n th√†nh kh√¥ng c√≥ NaN/Inf
- [ ] Checkpoint ƒë√£ l∆∞u ƒë·ªÉ fine-tuning n·∫øu c·∫ßn
- [ ] Metrics ƒë√£ log ƒë·ªÉ so s√°nh
- [ ] Evaluation report ƒë√£ l∆∞u (`artifacts/cf/{model}/eval_results.json`)
- [ ] Statistical significance test ƒë√£ pass (p < 0.05)
- [ ] Model ƒë√£ ƒëƒÉng k√Ω v√†o Registry
- [ ] Best model ƒë√£ ƒë∆∞·ª£c select (n·∫øu metrics c·∫£i thi·ªán)
- [ ] Audit log ƒë√£ ghi nh·∫≠n thao t√°c
