"""
Content Enrichment Module for Vietnamese Cosmetics Recommender System

This module is responsible for cleaning, enriching, and formatting product metadata
before feeding it into a BERT model for embedding generation.

Author: Senior Python Data Engineer
Date: 2025-11-22
"""

import logging
import numpy as np
import pandas as pd
from typing import List, Optional
from pathlib import Path

# Configure logger
logger = logging.getLogger("data_layer")


class ContentEnricher:
    """
    Content enrichment class for Vietnamese cosmetics product metadata.
    
    This class handles:
    - Standardization of Vietnamese skin type descriptions to English tags
    - Creation of rich-context BERT input text
    - Metadata enrichment with popularity scores
    - Data persistence in Parquet format
    """
    
    def __init__(self):
        """Initialize the ContentEnricher with Vietnamese-to-English skin type mappings."""
        # Skin type keyword mappings (Vietnamese -> English)
        self.skin_type_mapping = {
            'má»¥n': 'acne',
            'trá»©ng cÃ¡': 'acne',
            'dáº§u': 'oily',
            'nhá»n': 'oily',
            'khÃ´': 'dry',
            'há»—n há»£p': 'combination',
            'nháº¡y cáº£m': 'sensitive',
            'tráº¯ng': 'whitening',
            'sÃ¡ng': 'whitening',
            'lÃ£o hÃ³a': 'anti_aging',
            'nhÄƒn': 'anti_aging'
        }
        logger.info("ContentEnricher initialized with skin type mappings")
    
    def normalize_product_name(self, name: str) -> str:
        """
        Normalize product name for robust string matching during merge.
        
        Handles:
        - Case inconsistencies (uppercase/lowercase)
        - Extra whitespace (leading/trailing/multiple spaces)
        - Empty strings
        
        Args:
            name: Raw product name string
            
        Returns:
            Normalized lowercase product name with single spaces
            
        Examples:
            >>> enricher.normalize_product_name("  Kem  DÆ°á»¡ng  Da  ")
            'kem dÆ°á»¡ng da'
        """
        if pd.isna(name) or not str(name).strip():
            return ""
        # Convert to lowercase, strip edges, collapse multiple spaces to single
        normalized = str(name).lower().strip()
        normalized = ' '.join(normalized.split())  # Collapse multiple spaces
        return normalized
    
    def standardize_skin_type(self, raw_text: str) -> List[str]:
        """
        Normalize free-text Vietnamese skin type descriptions into standardized English tags.
        
        Args:
            raw_text: Raw Vietnamese text describing skin type(s)
            
        Returns:
            List of standardized English skin type tags
            
        Examples:
            >>> enricher.standardize_skin_type("Da dáº§u má»¥n")
            ['oily', 'acne']
            >>> enricher.standardize_skin_type(None)
            ['all_types']
        """
        # Handle NaN or empty values
        if pd.isna(raw_text) or not str(raw_text).strip():
            return ['all_types']
        
        # Convert to lowercase for case-insensitive matching
        text_lower = str(raw_text).lower()
        
        # Extract matching skin types
        detected_types = set()
        for vietnamese_keyword, english_tag in self.skin_type_mapping.items():
            if vietnamese_keyword in text_lower:
                detected_types.add(english_tag)
        
        # Return detected types or fallback to 'all_types'
        if detected_types:
            return sorted(list(detected_types))  # Sort for consistency
        else:
            return ['all_types']
    
    def create_bert_input_text_vectorized(self, df: pd.DataFrame) -> pd.Series:
        """
        Create rich-context BERT input text using VECTORIZED operations for performance.
        
        This method is 50-100x faster than row-by-row apply() on large datasets.
        
        Args:
            df: DataFrame containing product records
            
        Returns:
            Series of formatted BERT input strings with [SEP] separators
            
        Format:
            "TÃªn: {product_name} [SEP] ThÆ°Æ¡ng hiá»‡u: {brand} [SEP] 
             ThÃ nh pháº§n: {ingredient} [SEP] CÃ´ng dá»¥ng: {feature} [SEP] 
             Loáº¡i da: {skin_type} [SEP] MÃ´ táº£: {processed_description}"
        """
        # Helper function to safely prepare column values
        def prepare_column(col_name: str, default: str = "KhÃ´ng rÃµ") -> pd.Series:
            if col_name not in df.columns:
                return pd.Series([default] * len(df), index=df.index)
            # Fill NaN, convert to string, strip whitespace, replace empty with default
            col = df[col_name].fillna(default).astype(str).str.strip()
            return col.replace('', default)
        
        # Prepare all columns with vectorized operations
        product_name = prepare_column('product_name')
        brand = prepare_column('brand')
        ingredient = prepare_column('ingredient')
        feature = prepare_column('feature')
        skin_type = prepare_column('skin_type')
        description = prepare_column('processed_description')
        
        # Build BERT input using vectorized string concatenation
        bert_input = (
            "TÃªn: " + product_name + " [SEP] " +
            "ThÆ°Æ¡ng hiá»‡u: " + brand + " [SEP] " +
            "ThÃ nh pháº§n: " + ingredient + " [SEP] " +
            "CÃ´ng dá»¥ng: " + feature + " [SEP] " +
            "Loáº¡i da: " + skin_type + " [SEP] " +
            "MÃ´ táº£: " + description
        )
        
        return bert_input
    
    def enrich_data(
        self, 
        products_df: pd.DataFrame, 
        attributes_df: pd.DataFrame
    ) -> pd.DataFrame:
        """
        Enrich product data by merging with attributes and adding computed fields.
        
        Args:
            products_df: DataFrame containing base product information
            attributes_df: DataFrame containing product attributes and metadata
            
        Returns:
            Enriched DataFrame with standardized skin types, BERT input text, and popularity scores
            
        Processing Steps:
            1. Remove duplicates from attributes (FIX #1)
            2. Normalize product_name for robust merge (FIX #2)
            3. Smart merge on product_id or normalized product_name (fallback)
            4. Coalesce overlapping columns to preserve all available data (FIX #3)
            5. Standardize skin types
            6. Create BERT input text (vectorized)
            7. Compute popularity scores
        """
        logger.info(f"Starting enrichment: {len(products_df)} products, {len(attributes_df)} attributes")
        
        # Step 0: Prepare data - remove duplicates and normalize
        # Create working copies to avoid modifying originals
        products_work = products_df.copy()
        attributes_work = attributes_df.copy()
        
        # FIX #1: Remove duplicates from attributes (CRITICAL: prevents row multiplication)
        original_attr_count = len(attributes_work)
        if 'product_name' in attributes_work.columns:
            # First normalize product_name for accurate duplicate detection
            attributes_work['_product_name_normalized'] = attributes_work['product_name'].apply(
                self.normalize_product_name
            )
            # Remove duplicates based on NORMALIZED product_name, keep first occurrence
            attributes_work = attributes_work.drop_duplicates(
                subset=['_product_name_normalized'], 
                keep='first'
            )
            removed_dupes = original_attr_count - len(attributes_work)
            if removed_dupes > 0:
                logger.warning(
                    f"âš ï¸  Removed {removed_dupes} duplicate rows from attributes "
                    f"({original_attr_count} â†’ {len(attributes_work)} unique products)"
                )
        
        # FIX #2: Apply normalize_product_name to BOTH datasets before merge
        # This ensures robust matching even with case differences and extra whitespace
        
        # Step 1: Smart merge with fallback logic
        # Check if product_id values overlap between datasets
        products_ids = set(products_work['product_id'].dropna())
        attributes_ids = set(attributes_work['product_id'].dropna())
        id_overlap = len(products_ids & attributes_ids)
        
        if id_overlap > 0:
            # IDs match - merge on product_id
            logger.info(f"Merging on 'product_id' (found {id_overlap} overlapping IDs)")
            enriched_df = products_work.merge(
                attributes_work, 
                on='product_id', 
                how='left',
                suffixes=('', '_attr')
            )
        elif 'product_name' in products_work.columns and 'product_name' in attributes_work.columns:
            # IDs don't match - fallback to NORMALIZED product_name merge
            logger.warning(
                f"âš ï¸  No product_id overlap detected! "
                f"Falling back to merge on normalized 'product_name'. "
                f"Products IDs: {list(products_ids)[:3]}..., "
                f"Attributes IDs: {list(attributes_ids)[:3]}..."
            )
            
            # Normalize product names for robust matching (FIX #2)
            # Note: attributes_work already has '_product_name_normalized' from Step 0
            products_work['_product_name_normalized'] = products_work['product_name'].apply(
                self.normalize_product_name
            )
            
            # Check match rate before merge
            products_names = set(products_work['_product_name_normalized'].dropna())
            attributes_names = set(attributes_work['_product_name_normalized'].dropna())
            name_overlap = len(products_names & attributes_names)
            match_rate = (name_overlap / len(products_names)) * 100 if len(products_names) > 0 else 0
            logger.info(
                f"Product name match rate: {name_overlap}/{len(products_names)} ({match_rate:.1f}%)"
            )
            
            # Merge on normalized names
            enriched_df = products_work.merge(
                attributes_work, 
                on='_product_name_normalized', 
                how='left',
                suffixes=('', '_attr')
            )
            
            # Clean up: keep original product_name from products, remove normalized column
            if 'product_name_attr' in enriched_df.columns:
                enriched_df.drop(columns=['product_name_attr'], inplace=True)
            enriched_df.drop(columns=['_product_name_normalized'], inplace=True)
            
            # Update product_id to use the attributes' version (real marketplace ID)
            if 'product_id_attr' in enriched_df.columns:
                enriched_df['product_id'] = enriched_df['product_id_attr'].fillna(enriched_df['product_id'])
                enriched_df.drop(columns=['product_id_attr'], inplace=True)
        else:
            raise ValueError(
                "Cannot merge: 'product_id' values don't overlap and 'product_name' not found in both datasets"
            )
        
        logger.info(f"Merged data: {len(enriched_df)} records")
        
        # Validation: Check for unexpected row multiplication
        if len(enriched_df) > len(products_work):
            excess_rows = len(enriched_df) - len(products_work)
            logger.error(
                f"ðŸš¨ CRITICAL: Merge created {excess_rows} duplicate rows! "
                f"({len(products_work)} products â†’ {len(enriched_df)} merged records). "
                f"This indicates remaining duplicates in attributes data."
            )
            raise ValueError(
                f"Data integrity error: Merge resulted in row multiplication "
                f"({len(products_work)} â†’ {len(enriched_df)}). "
                f"Check for duplicates in attributes data."
            )
        
        # FIX #3: Coalesce overlapping columns (preserve data from both sources)
        # Brand coalescing: use attributes brand if products brand is missing/generic
        if 'brand' in enriched_df.columns and 'brand_attr' in enriched_df.columns:
            # Fill missing brands from attributes, prefer attributes if products has generic values
            mask_missing = enriched_df['brand'].isna() | enriched_df['brand'].isin(['no_brand', 'No Brand', ''])
            filled_count = mask_missing.sum()
            enriched_df.loc[mask_missing, 'brand'] = enriched_df.loc[mask_missing, 'brand_attr']
            enriched_df.drop(columns=['brand_attr'], inplace=True)
            logger.info(f"âœ“ Coalesced 'brand' column: filled {filled_count} missing/generic values from attributes")
        
        # Apply same logic to other potentially overlapping columns
        for col in ['ingredient', 'feature', 'skin_type']:
            attr_col = f"{col}_attr"
            if col in enriched_df.columns and attr_col in enriched_df.columns:
                mask_missing = enriched_df[col].isna() | (enriched_df[col].astype(str).str.strip() == '')
                filled_count = mask_missing.sum()
                if filled_count > 0:
                    enriched_df.loc[mask_missing, col] = enriched_df.loc[mask_missing, attr_col]
                    logger.info(f"âœ“ Coalesced '{col}' column: filled {filled_count} missing values from attributes")
                enriched_df.drop(columns=[attr_col], inplace=True)
        
        # Step 2: Standardize skin types
        if 'skin_type' in enriched_df.columns:
            enriched_df['skin_type_standardized'] = enriched_df['skin_type'].apply(
                self.standardize_skin_type
            )
            logger.info("Skin type standardization complete")
        else:
            logger.warning("'skin_type' column not found, creating default values")
            enriched_df['skin_type_standardized'] = [['all_types']] * len(enriched_df)
        
        # Step 3: Create BERT input text using VECTORIZED operations (50-100x faster)
        enriched_df['bert_input_text'] = self.create_bert_input_text_vectorized(enriched_df)
        logger.info("BERT input text generation complete (vectorized)")
        
        # Step 4: Compute popularity score
        if 'num_sold_time' in enriched_df.columns:
            # Fill NaN values with 0 before log transformation
            enriched_df['num_sold_time'] = enriched_df['num_sold_time'].fillna(0)
            enriched_df['popularity_score'] = np.log1p(enriched_df['num_sold_time'])
            logger.info("Popularity scores computed")
        else:
            logger.warning("'num_sold_time' column not found, setting popularity_score to 0")
            enriched_df['popularity_score'] = 0.0
        
        logger.info(f"Enrichment complete: {len(enriched_df)} enriched records")
        return enriched_df
    
    def process_and_save(
        self, 
        products_path: str, 
        attributes_path: str, 
        output_path: str
    ) -> None:
        """
        Complete pipeline: load CSVs, enrich data, and save to Parquet format.
        
        Args:
            products_path: Path to products CSV file
            attributes_path: Path to attributes CSV file
            output_path: Path where enriched Parquet file will be saved
            
        Raises:
            FileNotFoundError: If input files don't exist
            ValueError: If required columns are missing
        """
        try:
            # Load products data
            logger.info(f"Loading products from: {products_path}")
            products_df = pd.read_csv(products_path, encoding='utf-8')
            logger.info(f"Loaded {len(products_df)} products")
            
            # Load attributes data
            logger.info(f"Loading attributes from: {attributes_path}")
            attributes_df = pd.read_csv(attributes_path, encoding='utf-8')
            logger.info(f"Loaded {len(attributes_df)} attribute records")
            
            # Validate required columns
            if 'product_id' not in products_df.columns:
                raise ValueError("'product_id' column missing in products_df")
            if 'product_id' not in attributes_df.columns:
                raise ValueError("'product_id' column missing in attributes_df")
            
            # Enrich data
            logger.info("Starting data enrichment process...")
            enriched_df = self.enrich_data(products_df, attributes_df)
            
            # Create output directory if it doesn't exist
            output_dir = Path(output_path).parent
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # Save to Parquet format
            logger.info(f"Saving enriched data to: {output_path}")
            enriched_df.to_parquet(output_path, index=False, engine='pyarrow')
            
            logger.info(f"âœ“ Enrichment pipeline complete. Output saved to {output_path}")
            logger.info(f"  - Total records: {len(enriched_df)}")
            logger.info(f"  - Columns: {list(enriched_df.columns)}")
            
        except FileNotFoundError as e:
            logger.error(f"File not found: {e}")
            raise
        except Exception as e:
            logger.error(f"Error during processing: {e}")
            raise


def main():
    """
    Main execution block for testing the ContentEnricher module.
    
    This demonstrates the complete pipeline with sample data paths.
    Adjust paths according to your project structure.
    """
    # Configure logging for testing
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Define paths (adjust to your project structure)
    base_path = Path(__file__).parent.parent.parent.parent.parent
    products_path = base_path / "data" / "published_data" / "data_product.csv"
    # Use merged file with sequential product_id (0-2243) and extra columns (type, skin_kind, is_5_star, num_sold_time, price)
    attributes_path = base_path / "data" / "published_data" / "attribute_based_embeddings" / "attribute_text_filtering_merged.csv"
    output_path = base_path / "data" / "processed" / "enriched_products.parquet"
    
    # Initialize enricher
    enricher = ContentEnricher()
    
    # Run the pipeline
    try:
        enricher.process_and_save(
            products_path=str(products_path),
            attributes_path=str(attributes_path),
            output_path=str(output_path)
        )
        print("\nâœ“ Content enrichment pipeline executed successfully!")
        
    except Exception as e:
        print(f"\nâœ— Pipeline failed: {e}")
        logger.exception("Full traceback:")


if __name__ == "__main__":
    main()
